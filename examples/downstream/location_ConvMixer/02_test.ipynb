{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "tf.config.experimental.set_visible_devices(physical_devices[3], 'GPU')\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import warnings\n",
    "import contextlib\n",
    "import multiprocessing\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import signal\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, Flatten, Reshape, Activation,\n",
    "    Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization,\n",
    "    Add, concatenate, DepthwiseConv1D\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "\n",
    "# External utilities (as in your original code)\n",
    "from EqT_utils_Loc_California import DataGenerator, _lr_schedule\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a9acd",
   "metadata": {},
   "source": [
    "## Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33224d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_depth_rate = 0.1\n",
    "positional_emb = False\n",
    "conv_layers = 1\n",
    "num_classes = 1\n",
    "projection_dim = 80\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim, projection_dim]\n",
    "transformer_layers = 4\n",
    "\n",
    "\n",
    "def convF1(inpt, D1, fil_ord, Dr):\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    #filters = inpt._keras_shape[channel_axis]\n",
    "    filters = int(inpt.shape[-1])\n",
    "    \n",
    "    #infx = Activation(tf.nn.gelu')(inpt)\n",
    "    pre = Conv1D(filters,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(inpt)\n",
    "    pre = BatchNormalization()(pre)    \n",
    "    pre = Activation('linear')(pre)\n",
    "    \n",
    "    #shared_conv = Conv1D(D1,  fil_ord, strides =(1), padding='same')\n",
    "    \n",
    "    inf  = Conv1D(filters,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(pre)\n",
    "    inf = BatchNormalization()(inf)    \n",
    "    inf = Activation('linear')(inf)\n",
    "    inf = Add()([inf,inpt])\n",
    "    \n",
    "    inf1  = Conv1D(D1,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(inf)\n",
    "    inf1 = BatchNormalization()(inf1)  \n",
    "    inf1 = Activation('linear')(inf1)    \n",
    "    encode = Dropout(Dr)(inf1, training=False)\n",
    "\n",
    "    return encode\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n",
    "\n",
    "\n",
    "class CCTTokenizer1(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=(1,1,1,1),\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[int(projection_dim)] * 8,\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer1, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_model = tf.keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv1D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        reshaped = tf.reshape(outputs, (-1, tf.shape(outputs)[1], tf.shape(outputs)[2]))\n",
    "        return outputs\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, 1))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = int(dummy_outputs.shape[1])\n",
    "            projection_dim = int(dummy_outputs.shape[-1])\n",
    "\n",
    "            embed_layer = layers.Embedding(input_dim=sequence_length, output_dim=projection_dim)\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def create_cct_model1(inputs):\n",
    "    cct_tokenizer = CCTTokenizer1()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    return representation\n",
    "\n",
    "\n",
    "def UNET(inputs, D1):\n",
    "    D2 = int(D1 * 2)\n",
    "    D3 = int(D2 * 2)\n",
    "    D4 = int(D3 * 2)\n",
    "    D5 = int(D4 * 2)\n",
    "\n",
    "    conv1 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=(2))(conv1)\n",
    "\n",
    "    conv2 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=(2))(conv2)\n",
    "\n",
    "    conv3 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=(2))(conv3)\n",
    "\n",
    "    conv4 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=(2))(conv4)\n",
    "\n",
    "    conv44 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv44 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv44)\n",
    "    pool44 = MaxPooling1D(pool_size=(5))(conv44)\n",
    "\n",
    "    conv5 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool44)\n",
    "    conv5 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    drop5 = create_cct_model1(conv5)\n",
    "\n",
    "    up66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(5))(drop5))\n",
    "    merge66 = concatenate([pool4, up66], axis=-1)\n",
    "    conv66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge66)\n",
    "    conv66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv66)\n",
    "\n",
    "    up6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv66))\n",
    "    merge6 = concatenate([conv4, up6], axis=-1)\n",
    "    conv6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv1D(D3, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=-1)\n",
    "    conv7 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv1D(D2, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=-1)\n",
    "    conv8 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv1D(D1, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=-1)\n",
    "    conv9 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    return conv9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269487b",
   "metadata": {},
   "source": [
    "## ConvMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (6000,3)\n",
    "dropout= 0.1\n",
    "def activation_block(x):\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    if dropout != 0.0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_stem(x, filters: int, patch_size: int):\n",
    "    x = layers.Conv1D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
    "    return activation_block(x)\n",
    "\n",
    "\n",
    "def conv_mixer_block(x, filters: int, kernel_size: int):\n",
    "    # Depthwise convolution.\n",
    "    x0 = x\n",
    "    x = DepthwiseConv1D(kernel_size=kernel_size, padding=\"same\")(x)\n",
    "    x = layers.Add()([activation_block(x), x0])  # Residual.\n",
    "\n",
    "    # Pointwise convolution.\n",
    "    x = layers.Conv1D(filters, kernel_size=1)(x)\n",
    "    x = activation_block(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_conv_mixer_256_8(\n",
    "\n",
    "    inputs,inp,image_size=6000, filters=512, depth=10, kernel_size=13, patch_size=10, num_classes=3  #\n",
    "    \n",
    "):\n",
    "    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n",
    "    The hyperparameter values are taken from the paper.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ## Decoder of the Foundation Encoder\n",
    "    x = convF1(inputs,   80, 13, 0.1)\n",
    "    x = convF1(x,   80, 13, 0.1)\n",
    "    x = convF1(x,   80, 13, 0.1)\n",
    "    x = Flatten()(x)\n",
    "    x =  Reshape((6000,1))(x)\n",
    "    ## Ready to Concatenate\n",
    "    x = concatenate([x,inp])\n",
    "        \n",
    "    x = conv_stem(x, filters, patch_size)\n",
    "    \n",
    "    # ConvMixer blocks.\n",
    "    for _ in range(depth):\n",
    "        x = conv_mixer_block(x, filters, kernel_size)\n",
    "\n",
    "    # Classification block.\n",
    "    x = layers.GlobalAvgPool1D()(x)\n",
    "\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe8a63",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc41b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "from EqT_utils_Loc_California import DataGeneratorTest\n",
    "\n",
    "\n",
    "\n",
    "def picker(args, yh3, yh3_std, spt=None):\n",
    "    \"\"\" \n",
    "    \n",
    "    Performs detection and picking.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : dic\n",
    "        A dictionary containing all of the input parameters.  \n",
    "        \n",
    "        \n",
    "    yh3 : 1D array\n",
    "        P arrival probabilities.  \n",
    "        \n",
    "    yh3_std : 1D array\n",
    "        P arrival standard deviations.  \n",
    "        \n",
    "        \n",
    "    spt : {int, None}, default=None    \n",
    "        P arrival time in sample.\n",
    "        \n",
    "        \n",
    "    \"\"\"               \n",
    "        \n",
    "\n",
    "\n",
    "    P_PICKall=[]\n",
    "    Ppickall=[]\n",
    "    Pproball = []\n",
    "    perrorall=[]\n",
    "\n",
    "\n",
    "\n",
    "    sP_arr = _detect_peaks(yh3, mph=args['S_threshold'], mpd=1)\n",
    "\n",
    "    P_PICKS = []\n",
    "    pick_errors = []\n",
    "    if len(sP_arr) > 0:\n",
    "        P_uncertainty = None  \n",
    "\n",
    "        for pick in range(len(sP_arr)):        \n",
    "            sauto = sP_arr[pick]\n",
    "\n",
    "            if  args['estimate_uncertainty'] and sauto:\n",
    "                P_uncertainty = np.round(yh3_std[int(sauto)], 3)\n",
    "\n",
    "            if sauto: \n",
    "                P_prob = np.round(yh3[int(sauto)], 3) \n",
    "                P_PICKS.append([sauto,P_prob, P_uncertainty]) \n",
    "\n",
    "    so=[]\n",
    "    si=[]\n",
    "    P_PICKS = np.array(P_PICKS)\n",
    "    P_PICKall.append(P_PICKS)\n",
    "    for ij in P_PICKS:\n",
    "        so.append(ij[1])\n",
    "        si.append(ij[0])\n",
    "    try:\n",
    "        so = np.array(so)\n",
    "        inds = np.argmax(so)\n",
    "        swave = si[inds]\n",
    "        perrorall.append(int(spt- swave))  \n",
    "        Ppickall.append(int(swave))\n",
    "        Pproball.append(int(np.max(so)))\n",
    "    except:\n",
    "        perrorall.append(None)\n",
    "        Ppickall.append(None)\n",
    "        Pproball.append(None)\n",
    "\n",
    "\n",
    "    #Ppickall = np.array(Ppickall)\n",
    "    #perrorall = np.array(perrorall)  \n",
    "    #Pproball = np.array(Pproball)\n",
    "    \n",
    "    return Ppickall, perrorall, Pproball\n",
    "\n",
    "def _detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising', kpsh=False, valley=False):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Detect peaks in data based on their amplitude and other features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D array_like\n",
    "        data.\n",
    "        \n",
    "    mph : {None, number}, default=None\n",
    "        detect peaks that are greater than minimum peak height.\n",
    "        \n",
    "    mpd : int, default=1\n",
    "        detect peaks that are at least separated by minimum peak distance (in number of data).\n",
    "        \n",
    "    threshold : int, default=0\n",
    "        detect peaks (valleys) that are greater (smaller) than `threshold in relation to their immediate neighbors.\n",
    "        \n",
    "    edge : str, default=rising\n",
    "        for a flat peak, keep only the rising edge ('rising'), only the falling edge ('falling'), both edges ('both'), or don't detect a flat peak (None).\n",
    "        \n",
    "    kpsh : bool, default=False\n",
    "        keep peaks with same height even if they are closer than `mpd`.\n",
    "        \n",
    "    valley : bool, default=False\n",
    "        if True (1), detect valleys (local minima) instead of peaks.\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    ind : 1D array_like\n",
    "        indeces of the peaks in `x`.\n",
    "\n",
    "    Modified from \n",
    "   ----------------\n",
    "    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.atleast_1d(x).astype('float64')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    # handle NaN's\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    # handle NaN's\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size-1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                    & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    return ind\n",
    "\n",
    "\n",
    "def generate_arrays_from_file(file_list, step):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Make a generator to generate list of trace names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_list : str\n",
    "        A list of trace names.  \n",
    "        \n",
    "    step : int\n",
    "        Batch size.  \n",
    "        \n",
    "    Returns\n",
    "    --------  \n",
    "    chunck : str\n",
    "        A batch of trace names. \n",
    "        \n",
    "    \"\"\"     \n",
    "    \n",
    "    n_loops = int(np.ceil(len(file_list) / step))\n",
    "    b = 0\n",
    "    while True:\n",
    "        for i in range(n_loops):\n",
    "            e = i*step + step \n",
    "            if e > len(file_list):\n",
    "                e = len(file_list)\n",
    "            chunck = file_list[b:e]\n",
    "            b=e\n",
    "            yield chunck   \n",
    "    \n",
    "def tester1(input_hdf5=None,\n",
    "           input_testset=None,\n",
    "           input_model=None,\n",
    "           output_name=None,\n",
    "           detection_threshold=0.20,                \n",
    "           p_threshold=0.1,\n",
    "           S_threshold=0.1, \n",
    "           number_of_plots=100,\n",
    "           estimate_uncertainty=True, \n",
    "           number_of_sampling=5,\n",
    "           loss_weights=[0.05, 0.40, 0.55],\n",
    "           loss_types=['mse'],\n",
    "           input_dimention=(6000, 3),\n",
    "           normalization_mode='std',\n",
    "           mode='generator',\n",
    "           batch_size=500,\n",
    "           gpuid=None,\n",
    "           gpu_limit=None):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Applies a trained model to a windowed waveform to perform both detection and picking at the same time.  \n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_hdf5: str, default=None\n",
    "        Path to an hdf5 file containing only one class of \"data\" with NumPy arrays containing 3 component waveforms each 1 min long.\n",
    "\n",
    "    input_testset: npy, default=None\n",
    "        Path to a NumPy file (automaticaly generated by the trainer) containing a list of trace names.        \n",
    "\n",
    "    input_model: str, default=None\n",
    "        Path to a trained model.\n",
    "        \n",
    "    output_dir: str, default=None\n",
    "        Output directory that will be generated. \n",
    "        \n",
    "    output_probabilities: bool, default=False\n",
    "        If True, it will output probabilities and estimated uncertainties for each trace into an HDF file. \n",
    "       \n",
    "          \n",
    "    S_threshold: float, default=0.1\n",
    "        A value which the P probabilities above it will be considered as P arrival.\n",
    "\n",
    "               \n",
    "    number_of_plots: float, default=10\n",
    "        The number of plots for detected events outputed for each station data.\n",
    "        \n",
    "    estimate_uncertainty: bool, default=False\n",
    "        If True uncertainties in the output probabilities will be estimated.  \n",
    "        \n",
    "    number_of_sampling: int, default=5\n",
    "        Number of sampling for the uncertainty estimation. \n",
    "               \n",
    "             \n",
    "    input_dimention: tuple, default=(6000, 3)\n",
    "        Loss types for P picking.          \n",
    "\n",
    "    normalization_mode: str, default='std' \n",
    "        Mode of normalization for data preprocessing, 'max', maximum amplitude among three components, 'std', standard deviation.\n",
    "\n",
    "    mode: str, default='generator'\n",
    "        Mode of running. 'pre_load_generator' or 'generator'.\n",
    "                      \n",
    "    batch_size: int, default=500 \n",
    "        Batch size. This wont affect the speed much but can affect the performance.\n",
    "\n",
    "    gpuid: int, default=None\n",
    "        Id of GPU used for the prediction. If using CPU set to None.\n",
    "         \n",
    "    gpu_limit: int, default=None\n",
    "        Set the maximum percentage of memory usage for the GPU.\n",
    "        \n",
    "\n",
    "    \"\"\" \n",
    "              \n",
    "         \n",
    "    args = {\n",
    "    \"input_hdf5\": input_hdf5,\n",
    "    \"input_testset\": input_testset,\n",
    "    \"input_model\": input_model,\n",
    "    \"output_name\": output_name,\n",
    "    \"detection_threshold\": detection_threshold,\n",
    "    \"S_threshold\": S_threshold,\n",
    "    \"number_of_plots\": number_of_plots,\n",
    "    \"estimate_uncertainty\": estimate_uncertainty,\n",
    "    \"number_of_sampling\": number_of_sampling,\n",
    "    \"input_dimention\": input_dimention,\n",
    "    \"normalization_mode\": normalization_mode,\n",
    "    \"mode\": mode,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"gpuid\": gpuid,\n",
    "    \"gpu_limit\": gpu_limit\n",
    "    }  \n",
    "\n",
    "    \n",
    "    if args['gpuid']:           \n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])\n",
    "        tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit']) \n",
    "        K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "    \n",
    "    save_dir = os.path.join(os.getcwd(), str(args['output_name'])+'_outputs')\n",
    "    save_figs = os.path.join(save_dir, 'figures')\n",
    " \n",
    "    if os.path.isdir(save_dir):\n",
    "        shutil.rmtree(save_dir)  \n",
    "    os.makedirs(save_figs) \n",
    " \n",
    "    test = np.load(args['input_testset'])\n",
    "    #test = test[0:1000]\n",
    "\n",
    "    print('Loading the model ...', flush=True) \n",
    "\n",
    "    D1 = 5\n",
    "    D2 = int(D1*2)\n",
    "    D3 = int(D2*2)\n",
    "    D4 = int(D3*2)\n",
    "    D5 = int(D4*2)\n",
    "\n",
    "    inp = Input(shape=input_shape,name=\"input\")\n",
    "    conv1 = UNET(inp,D1)\n",
    "    out = Conv1D(D1,  3, strides =(1), padding='same',kernel_initializer='he_normal')(conv1)\n",
    "    out = Conv1D(3,  3, strides =(1), padding='same',kernel_initializer='he_normal',name='picker_PP')(out)\n",
    "    modeloriginal = Model(inp, out)\n",
    "\n",
    "    modeloriginal.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc',f1,precision, recall])     \n",
    "    modeloriginal.load_weights('../../../weights/UTrans_Foundation.h5')\n",
    "\n",
    "    # Model CCT\n",
    "    inputs = modeloriginal.layers[63].output  # layer that you want to connect your new FC layer to \n",
    "\n",
    "    features = get_conv_mixer_256_8(inputs,inp)\n",
    "    #features = Reshape((6000,1))(features)\n",
    "    features = Flatten()(features)\n",
    "    outputs = layers.Dense(3, activation=\"linear\",name='output_layer')(features)\n",
    "\n",
    "    model = Model(inputs=modeloriginal.input, outputs=outputs)\n",
    "\n",
    "    #Adm = tensorflow.optimizers.Adam(lr=1e-4)\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])   \n",
    "        \n",
    "\n",
    "    model.load_weights(args['input_model'])\n",
    "\n",
    "\n",
    "    model.summary()  \n",
    "        \n",
    "    print('Loading is complete!', flush=True)  \n",
    "    print('Testing ...', flush=True)    \n",
    "    print('Writting results into: \" ' + str(args['output_name'])+'_outputs'+' \"', flush=True)\n",
    "    \n",
    "    start_training = time.time()          \n",
    "\n",
    "    csvTst = open(os.path.join(save_dir,'X_test_results.csv'), 'w')          \n",
    "    test_writer = csv.writer(csvTst, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    test_writer.writerow([\n",
    "                          \n",
    "                          \n",
    "                          's_arrival_sample', \n",
    "                          \n",
    "\n",
    "                          \n",
    "                          'S_pick',\n",
    "                          'S_probability',\n",
    "                          'S_error'\n",
    "                          ])  \n",
    "    csvTst.flush()        \n",
    "        \n",
    "    plt_n = 0\n",
    "    list_generator = generate_arrays_from_file(test, args['batch_size']) \n",
    "    pred_SS_mean_all=[]\n",
    "    pred_SS_std_all=[]\n",
    "    sptall = []\n",
    "    pred_PPall = []\n",
    "    \n",
    "    pbar_test = tqdm(total= int(np.ceil(len(test)/args['batch_size'])))            \n",
    "    for _ in range(int(np.ceil(len(test) / args['batch_size']))):\n",
    "        pbar_test.update()\n",
    "        new_list = next(list_generator)\n",
    "\n",
    "        if args['mode'].lower() == 'pre_load_generator':                \n",
    "            params_test = {'dim': args['input_dimention'][0],\n",
    "                           'batch_size': len(new_list),\n",
    "                           'n_channels': args['input_dimention'][-1],\n",
    "                           'norm_mode': args['normalization_mode']}  \n",
    "            test_set={}\n",
    "\n",
    "\n",
    "        \n",
    "        else:       \n",
    "            params_test = {'file_name': str(args['input_hdf5']), \n",
    "                           'dim': args['input_dimention'][0],\n",
    "                           'batch_size': len(new_list),\n",
    "                           'n_channels': args['input_dimention'][-1],\n",
    "                           'norm_mode': args['normalization_mode']}     \n",
    "    \n",
    "            test_generator = DataGeneratorTest(new_list, **params_test)\n",
    "            \n",
    "            if args['estimate_uncertainty']:\n",
    "                pred_PP = []\n",
    "                for mc in range(args['number_of_sampling']):\n",
    "                    predP = model.predict_generator(generator=test_generator)\n",
    "                    pred_PP.append(predP)\n",
    "                \n",
    "    \n",
    "                \n",
    "                pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list),3)\n",
    "                pred_SS_mean = pred_PP.mean(axis=0)\n",
    "                pred_SS_std = pred_PP.std(axis=0) \n",
    "\n",
    "            else:          \n",
    "                pred_SS_mean = model.predict_generator(generator=test_generator)\n",
    "                pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1]) \n",
    "                \n",
    "                pred_SS_std = np.zeros((pred_SS_mean.shape))   \n",
    "                \n",
    "   \n",
    "            test_set={}\n",
    "            fl = h5py.File(args['input_hdf5'], 'r')\n",
    "            for ID in new_list:\n",
    "                dataset = fl.get(str(ID))\n",
    "                test_set.update( {str(ID) : dataset})                 \n",
    "            \n",
    "            for ts in range(pred_SS_mean.shape[0]): \n",
    "                evi =  new_list[ts] \n",
    "                dataset = test_set[evi]  \n",
    "                \n",
    "                loc = []   \n",
    "                try:\n",
    "                    loc.append(dataset.attrs['station_lat']);\n",
    "                    loc.append(dataset.attrs['station_long']);\n",
    "                    loc.append(dataset.attrs['event_lat']);\n",
    "                    loc.append(dataset.attrs['event_long']);\n",
    "                    loc.append(float(dataset.attrs['depth']));\n",
    "\n",
    "                except Exception:     \n",
    "                    loc = None\n",
    "                \n",
    "                \n",
    "                #Ppick, perror, Pprob =  picker(args, pred_SS_mean[ts], pred_SS_std[ts], spt) \n",
    "                \n",
    "                #_output_writter_test(args, dataset, evi, test_writer, csvTst, Ppick, perror, Pprob)\n",
    "                        \n",
    "                pred_PPall.append(pred_PP[:,ts])\n",
    "                pred_SS_mean_all.append(pred_SS_mean[ts])\n",
    "                pred_SS_std_all.append(pred_SS_std[ts])\n",
    "                sptall.append(loc)\n",
    "                \n",
    "\n",
    "    \n",
    "    #np.save('pred_SS_mean_all_Vit_Mag.npy',pred_SS_mean_all)\n",
    "    #np.save('pred_SS_std_all_Vit_Mag.npy',pred_SS_std_all)\n",
    "    np.save('true_FoundationLoc.npy',sptall)\n",
    "    np.save('pred_FoundationLoc.npy',pred_PPall)\n",
    "\n",
    "    end_training = time.time()  \n",
    "    delta = end_training - start_training\n",
    "    hour = int(delta / 3600)\n",
    "    delta -= hour * 3600\n",
    "    minute = int(delta / 60)\n",
    "    delta -= minute * 60\n",
    "    seconds = delta     \n",
    "                    \n",
    "    with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file: \n",
    "        the_file.write('================== Overal Info =============================='+'\\n')               \n",
    "                \n",
    "        the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\\n')            \n",
    "        the_file.write('input_testset: '+str(args['input_testset'])+'\\n')\n",
    "        the_file.write('input_model: '+str(args['input_model'])+'\\n')\n",
    "        the_file.write('output_name: '+str(args['output_name']+'_outputs')+'\\n')  \n",
    "        the_file.write('================== Testing Parameters ======================='+'\\n')  \n",
    "        the_file.write('mode: '+str(args['mode'])+'\\n')  \n",
    "        the_file.write('finished the test in:  {} hours and {} minutes and {} seconds \\n'.format(hour, minute, round(seconds, 3))) \n",
    "        the_file.write('batch_size: '+str(args['batch_size'])+'\\n')\n",
    "        the_file.write('total number of tests '+str(len(test))+'\\n')\n",
    "        the_file.write('gpuid: '+str(args['gpuid'])+'\\n')\n",
    "        the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\\n')             \n",
    "        the_file.write('================== Other Parameters ========================='+'\\n')            \n",
    "        the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\\n')\n",
    "        the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\\n')\n",
    "        the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\\n')                       \n",
    "        the_file.write('S_threshold: '+str(args['S_threshold'])+'\\n')\n",
    "        \n",
    "    \n",
    "    \n",
    "def _output_writter_test(args, \n",
    "                        dataset, \n",
    "                        evi, \n",
    "                        output_writer, \n",
    "                        csvfile, \n",
    "                        Ppick,\n",
    "                        perror,\n",
    "                        Pprob\n",
    "                        ):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Writes the detection & picking results into a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args: dic\n",
    "        A dictionary containing all of the input parameters.    \n",
    " \n",
    "    dataset: hdf5 obj\n",
    "        Dataset object of the trace.\n",
    "\n",
    "    evi: str\n",
    "        Trace name.    \n",
    "              \n",
    "    output_writer: obj\n",
    "        For writing out the detection/picking results in the CSV file.\n",
    "        \n",
    "    csvfile: obj\n",
    "        For writing out the detection/picking results in the CSV file.   \n",
    "             \n",
    "        \n",
    "    Returns\n",
    "    --------  \n",
    "    X_test_results.csv  \n",
    "    \n",
    "        \n",
    "    \"\"\"        \n",
    "    \n",
    "    \n",
    "    #print(dataset.attrs['data_category'] )\n",
    "    \n",
    "    if dataset.attrs['data_category'] != 'noise':                                     \n",
    "\n",
    "        s_arrival_sample = dataset.attrs['s_arrival_sample'] \n",
    "\n",
    "                   \n",
    "    elif dataset.attrs['data_category'] == 'noise':               \n",
    "        #network_code = dataset.attrs['network_code']\n",
    "        source_id = None\n",
    "        source_distance_km = None \n",
    "        snr_db = None\n",
    "        #trace_name = dataset.attrs['trace_name'] \n",
    "        #trace_category = dataset.attrs['trace_category']            \n",
    "        trace_start_time = None\n",
    "        source_magnitude = None\n",
    "        s_arrival_sample = None\n",
    "        S_status = None\n",
    "        S_weight = None\n",
    "        s_arrival_sample = None\n",
    "        s_status = None\n",
    "        s_weight = None\n",
    "        #receiver_type = dataset.attrs['receiver_type'] \n",
    "      \n",
    "    #print(Ppick[0])\n",
    "\n",
    "    output_writer.writerow([ \n",
    "\n",
    "                            s_arrival_sample, \n",
    "                             \n",
    "\n",
    "            \n",
    "                            Ppick[0], \n",
    "                            Pprob[0],\n",
    "                            perror[0],\n",
    "                            \n",
    "                            ]) \n",
    "    \n",
    "    csvfile.flush()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#`input_model: Path to the trained model directory: Change based on the best model you obtained\n",
    "#`input_hdf5`: Path to the full waveform HDF5 file.\n",
    "\n",
    "tester1(input_hdf5= '/scratch/sadalyom/DataCollected',\n",
    "       input_testset='test_Events.npy',\n",
    "       input_model = 'test_trainer_FoundationEqConvMixer_outputs/models/test_trainer_FoundationEqConvMixer_001.h5',\n",
    "       output_name='test_tester',\n",
    "       detection_threshold=0.1,                \n",
    "       p_threshold=0.1,\n",
    "       number_of_plots=3,\n",
    "       estimate_uncertainty=True, \n",
    "       number_of_sampling=1,\n",
    "       input_dimention=(6000, 3),\n",
    "       normalization_mode='std',\n",
    "       mode='generator',\n",
    "       batch_size=1024,\n",
    "       gpuid=None,\n",
    "       gpu_limit=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c30cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
