{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7660dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import warnings\n",
    "import contextlib\n",
    "import multiprocessing\n",
    "import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import signal\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, Flatten, Reshape, Activation,\n",
    "    Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization,\n",
    "    Add, concatenate, DepthwiseConv1D\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "\n",
    "# External utilities (as in your original code)\n",
    "from EqT_utils_Mag import DataGenerator, _lr_schedule\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eedb6f",
   "metadata": {},
   "source": [
    "## Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_depth_rate = 0.1\n",
    "positional_emb = False\n",
    "conv_layers = 1\n",
    "num_classes = 1\n",
    "projection_dim = 80\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim, projection_dim]\n",
    "transformer_layers = 4\n",
    "\n",
    "\n",
    "def convF1(inpt, D1, fil_ord, Dr):\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    #filters = inpt._keras_shape[channel_axis]\n",
    "    filters = int(inpt.shape[-1])\n",
    "    \n",
    "    #infx = Activation(tf.nn.gelu')(inpt)\n",
    "    pre = Conv1D(filters,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(inpt)\n",
    "    pre = BatchNormalization()(pre)    \n",
    "    pre = Activation('linear')(pre)\n",
    "    \n",
    "    #shared_conv = Conv1D(D1,  fil_ord, strides =(1), padding='same')\n",
    "    \n",
    "    inf  = Conv1D(filters,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(pre)\n",
    "    inf = BatchNormalization()(inf)    \n",
    "    inf = Activation('linear')(inf)\n",
    "    inf = Add()([inf,inpt])\n",
    "    \n",
    "    inf1  = Conv1D(D1,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(inf)\n",
    "    inf1 = BatchNormalization()(inf1)  \n",
    "    inf1 = Activation('linear')(inf1)    \n",
    "    encode = Dropout(Dr)(inf1, training=False)\n",
    "\n",
    "    return encode\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n",
    "\n",
    "\n",
    "class CCTTokenizer1(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=(1,1,1,1),\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[int(projection_dim)] * 8,\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer1, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_model = tf.keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv1D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        reshaped = tf.reshape(outputs, (-1, tf.shape(outputs)[1], tf.shape(outputs)[2]))\n",
    "        return outputs\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, 1))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = int(dummy_outputs.shape[1])\n",
    "            projection_dim = int(dummy_outputs.shape[-1])\n",
    "\n",
    "            embed_layer = layers.Embedding(input_dim=sequence_length, output_dim=projection_dim)\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def create_cct_model1(inputs):\n",
    "    cct_tokenizer = CCTTokenizer1()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    return representation\n",
    "\n",
    "\n",
    "def UNET(inputs, D1):\n",
    "    D2 = int(D1 * 2)\n",
    "    D3 = int(D2 * 2)\n",
    "    D4 = int(D3 * 2)\n",
    "    D5 = int(D4 * 2)\n",
    "\n",
    "    conv1 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=(2))(conv1)\n",
    "\n",
    "    conv2 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=(2))(conv2)\n",
    "\n",
    "    conv3 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=(2))(conv3)\n",
    "\n",
    "    conv4 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=(2))(conv4)\n",
    "\n",
    "    conv44 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv44 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv44)\n",
    "    pool44 = MaxPooling1D(pool_size=(5))(conv44)\n",
    "\n",
    "    conv5 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool44)\n",
    "    conv5 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    drop5 = create_cct_model1(conv5)\n",
    "\n",
    "    up66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(5))(drop5))\n",
    "    merge66 = concatenate([pool4, up66], axis=-1)\n",
    "    conv66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge66)\n",
    "    conv66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv66)\n",
    "\n",
    "    up6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv66))\n",
    "    merge6 = concatenate([conv4, up6], axis=-1)\n",
    "    conv6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv1D(D3, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=-1)\n",
    "    conv7 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv1D(D2, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=-1)\n",
    "    conv8 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv1D(D1, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=-1)\n",
    "    conv9 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    return conv9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec1fff",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4540780",
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_sampling = 50\n",
    "drop_rate = 0.2\n",
    "input_shape = (6000,3)\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "input_shapeX = (75, 32)\n",
    "image_sizeX = 75  # We'll resize input images to this size\n",
    "patch_sizeX = 5  # Size of the patches to be extract from the input images\n",
    "num_patchesX = (image_sizeX // patch_sizeX)\n",
    "projection_dimX = 100\n",
    "num_headsX = 4\n",
    "transformer_unitsX = [\n",
    "    projection_dimX * 2,\n",
    "    projection_dimX,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layersX = 4\n",
    "\n",
    "def mlpX(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        #x = layers.Dense(units, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class PatchesX(layers.Layer):\n",
    "    def __init__(self, patch_sizeX, **kwargs):\n",
    "        super(PatchesX, self).__init__()\n",
    "        self.patch_sizeX = patch_sizeX\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'patch_sizeX' : self.patch_sizeX, \n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config\n",
    "        \n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_sizeX, 1, 1],\n",
    "            strides=[1, self.patch_sizeX, 1, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "\n",
    "class PatchEncoderX(layers.Layer):\n",
    "    def __init__(self, num_patchesX, projection_dimX, **kwargs):\n",
    "        super(PatchEncoderX, self).__init__()\n",
    "        self.num_patchesX = num_patchesX\n",
    "        self.projection = layers.Dense(units=projection_dimX)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patchesX, output_dim=projection_dimX\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_patchesX' : self.num_patchesX, \n",
    "            'projection_dimX' : projection_dimX, \n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patchesX, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        \n",
    "        return encoded\n",
    "    \n",
    "def create_vit_classifier(inputs,inp):\n",
    "    \n",
    "    filters = [32, 64, 96, 128, 256] \n",
    "\n",
    "    x = convF1(inputs,   80, 13, 0.1)\n",
    "    x = convF1(x,   80, 13, 0.1)\n",
    "    x = convF1(x,   80, 13, 0.1)\n",
    "    x = Flatten()(x)\n",
    "    x =  Reshape((6000,1))(x)\n",
    "    x = concatenate([x,inp])\n",
    "    \n",
    "     \n",
    "    e = Conv1D(filters[1], 3, padding = 'same')(x) \n",
    "    e = Dropout(drop_rate)(e, training=False)\n",
    "    e = MaxPooling1D(2, padding='same')(e)\n",
    "\n",
    "    \n",
    "    e = Conv1D(filters[1], 3, padding = 'same')(e) \n",
    "    e = Dropout(drop_rate)(e, training=False)\n",
    "    e = MaxPooling1D(2, padding='same')(e)\n",
    "    \n",
    "    \n",
    "    e = Conv1D(filters[0], 3, padding = 'same')(e) \n",
    "    e = Dropout(drop_rate)(e, training=False)\n",
    "    e = MaxPooling1D(2, padding='same')(e)\n",
    "    \n",
    "    e = Conv1D(filters[0], 3, padding = 'same')(e) \n",
    "    e = Dropout(drop_rate)(e, training=False)\n",
    "    e = MaxPooling1D(2, padding='same')(e)\n",
    "    \n",
    "        \n",
    "    e = Conv1D(filters[0], 3, padding = 'same')(e) \n",
    "    e = Dropout(drop_rate)(e, training=False)\n",
    "    e = MaxPooling1D(5, padding='same')(e)\n",
    "    \n",
    "    \n",
    "    #print(e)\n",
    "    inputreshaped = layers.Reshape((75,1,32))(e)\n",
    "    # Create patches.\n",
    "    patches = PatchesX(patch_sizeX)(inputreshaped)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoderX(num_patchesX, projection_dimX)(patches)\n",
    "    \n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layersX):\n",
    "        #encoded_patches = convF1(encoded_PatchesX, projection_dimX,11, 0.1)\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_headsX, key_dim=projection_dimX, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # mlpX.\n",
    "        x3 = mlpX(x3, hidden_units=transformer_unitsX, dropout_rate=0.1)\n",
    "        #x3 = convF1(x3, projection_dimX,11, 0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dimX] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "\n",
    "    # Add mlpX.\n",
    "    features = mlpX(representation, hidden_units=[1000,500], dropout_rate=0.5)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53fe5a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d835d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from EqT_utils_Mag import DataGeneratorTest\n",
    "\n",
    "\n",
    "def generate_arrays_from_file(file_list, step):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Make a generator to generate list of trace names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_list : str\n",
    "        A list of trace names.  \n",
    "        \n",
    "    step : int\n",
    "        Batch size.  \n",
    "        \n",
    "    Returns\n",
    "    --------  \n",
    "    chunck : str\n",
    "        A batch of trace names. \n",
    "        \n",
    "    \"\"\"     \n",
    "    \n",
    "    n_loops = int(np.ceil(len(file_list) / step))\n",
    "    b = 0\n",
    "    while True:\n",
    "        for i in range(n_loops):\n",
    "            e = i*step + step \n",
    "            if e > len(file_list):\n",
    "                e = len(file_list)\n",
    "            chunck = file_list[b:e]\n",
    "            b=e\n",
    "            yield chunck   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def tester1(input_hdf5=None,\n",
    "           input_testset=None,\n",
    "           input_model=None,\n",
    "           output_name=None,\n",
    "           detection_threshold=0.20,                \n",
    "           p_threshold=0.1,\n",
    "           S_threshold=0.1, \n",
    "           number_of_plots=100,\n",
    "           estimate_uncertainty=True, \n",
    "           number_of_sampling=5,\n",
    "           loss_weights=[0.05, 0.40, 0.55],\n",
    "           loss_types=['mse'],\n",
    "           input_dimention=(6000, 3),\n",
    "           normalization_mode='std',\n",
    "           mode='generator',\n",
    "           batch_size=500,\n",
    "           gpuid=None,\n",
    "           gpu_limit=None):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Applies a trained model to a windowed waveform to perform both detection and picking at the same time.  \n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_hdf5: str, default=None\n",
    "        Path to an hdf5 file containing only one class of \"data\" with NumPy arrays containing 3 component waveforms each 1 min long.\n",
    "\n",
    "    input_testset: npy, default=None\n",
    "        Path to a NumPy file (automaticaly generated by the trainer) containing a list of trace names.        \n",
    "\n",
    "    input_model: str, default=None\n",
    "        Path to a trained model.\n",
    "        \n",
    "    output_dir: str, default=None\n",
    "        Output directory that will be generated. \n",
    "        \n",
    "    output_probabilities: bool, default=False\n",
    "        If True, it will output probabilities and estimated uncertainties for each trace into an HDF file. \n",
    "       \n",
    "          \n",
    "    S_threshold: float, default=0.1\n",
    "        A value which the P probabilities above it will be considered as P arrival.\n",
    "\n",
    "               \n",
    "    number_of_plots: float, default=10\n",
    "        The number of plots for detected events outputed for each station data.\n",
    "        \n",
    "    estimate_uncertainty: bool, default=False\n",
    "        If True uncertainties in the output probabilities will be estimated.  \n",
    "        \n",
    "    number_of_sampling: int, default=5\n",
    "        Number of sampling for the uncertainty estimation. \n",
    "               \n",
    "             \n",
    "    input_dimention: tuple, default=(6000, 3)\n",
    "        Loss types for P picking.          \n",
    "\n",
    "    normalization_mode: str, default='std' \n",
    "        Mode of normalization for data preprocessing, 'max', maximum amplitude among three components, 'std', standard deviation.\n",
    "\n",
    "    mode: str, default='generator'\n",
    "        Mode of running. 'pre_load_generator' or 'generator'.\n",
    "                      \n",
    "    batch_size: int, default=500 \n",
    "        Batch size. This wont affect the speed much but can affect the performance.\n",
    "\n",
    "    gpuid: int, default=None\n",
    "        Id of GPU used for the prediction. If using CPU set to None.\n",
    "         \n",
    "    gpu_limit: int, default=None\n",
    "        Set the maximum percentage of memory usage for the GPU.\n",
    "        \n",
    "\n",
    "    \"\"\" \n",
    "              \n",
    "         \n",
    "    args = {\n",
    "    \"input_hdf5\": input_hdf5,\n",
    "    \"input_testset\": input_testset,\n",
    "    \"input_model\": input_model,\n",
    "    \"output_name\": output_name,\n",
    "    \"detection_threshold\": detection_threshold,\n",
    "    \"S_threshold\": S_threshold,\n",
    "    \"number_of_plots\": number_of_plots,\n",
    "    \"estimate_uncertainty\": estimate_uncertainty,\n",
    "    \"number_of_sampling\": number_of_sampling,\n",
    "    \"input_dimention\": input_dimention,\n",
    "    \"normalization_mode\": normalization_mode,\n",
    "    \"mode\": mode,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"gpuid\": gpuid,\n",
    "    \"gpu_limit\": gpu_limit\n",
    "    }  \n",
    "\n",
    "    \n",
    "    if args['gpuid']:           \n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])\n",
    "        tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit']) \n",
    "        K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "    \n",
    "    save_dir = os.path.join(os.getcwd(), str(args['output_name'])+'_outputs')\n",
    "    save_figs = os.path.join(save_dir, 'figures')\n",
    " \n",
    "    if os.path.isdir(save_dir):\n",
    "        shutil.rmtree(save_dir)  \n",
    "    os.makedirs(save_figs) \n",
    " \n",
    "    test = np.load(args['input_testset'])\n",
    "\n",
    "    print('Loading the model ...', flush=True) \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    D1 = 5\n",
    "    D2 = int(D1*2)\n",
    "    D3 = int(D2*2)\n",
    "    D4 = int(D3*2)\n",
    "    D5 = int(D4*2)\n",
    "\n",
    "    inp = Input(shape=input_shape,name=\"input\")\n",
    "    conv1 = UNET(inp,D1)\n",
    "    out = Conv1D(D1,  3, strides =(1), padding='same',kernel_initializer='he_normal')(conv1)\n",
    "    out = Conv1D(3,  3, strides =(1), padding='same',kernel_initializer='he_normal',name='picker_PP')(out)\n",
    "    modeloriginal = Model(inp, out)\n",
    "\n",
    "    modeloriginal.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc',f1,precision, recall])     \n",
    "    #modeloriginal.load_weights('../../test_trainer_2_sym_outputs//models/test_trainer_2_sym_006.h5')\n",
    "\n",
    "    # Model CCT\n",
    "    inputs = modeloriginal.layers[63].output  # layer that you want to connect your new FC layer to \n",
    "\n",
    "    features = create_vit_classifier(inputs,inp)\n",
    "    #features = Reshape((6000,1))(features)\n",
    "\n",
    "    e = Dense(1)(features)\n",
    "    o = Activation('linear', name='output_layer')(e)\n",
    "    model = Model(inputs=modeloriginal.input, outputs=o)\n",
    "\n",
    "    #Adm = tensorflow.optimizers.Adam(lr=1e-4)\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])   \n",
    "        \n",
    "\n",
    "    model.load_weights(args['input_model'])\n",
    "\n",
    "\n",
    "    model.summary()  \n",
    "        \n",
    "    print('Loading is complete!', flush=True)  \n",
    "    print('Testing ...', flush=True)    \n",
    "    print('Writting results into: \" ' + str(args['output_name'])+'_outputs'+' \"', flush=True)\n",
    "    \n",
    "    start_training = time.time()          \n",
    "\n",
    "    csvTst = open(os.path.join(save_dir,'X_test_results.csv'), 'w')          \n",
    "    test_writer = csv.writer(csvTst, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    test_writer.writerow([\n",
    "                          \n",
    "                          \n",
    "                          's_arrival_sample', \n",
    "                          \n",
    "\n",
    "                          \n",
    "                          'S_pick',\n",
    "                          'S_probability',\n",
    "                          'S_error'\n",
    "                          ])  \n",
    "    csvTst.flush()        \n",
    "        \n",
    "    plt_n = 0\n",
    "    list_generator = generate_arrays_from_file(test, args['batch_size']) \n",
    "    pred_SS_mean_all=[]\n",
    "    pred_SS_std_all=[]\n",
    "    sptall = []\n",
    "    pred_PPall = []\n",
    "    \n",
    "    pbar_test = tqdm(total= int(np.ceil(len(test)/args['batch_size'])))            \n",
    "    for _ in range(int(np.ceil(len(test) / args['batch_size']))):\n",
    "        pbar_test.update()\n",
    "        new_list = next(list_generator)\n",
    "\n",
    "        if args['mode'].lower() == 'pre_load_generator':                \n",
    "            params_test = {'dim': args['input_dimention'][0],\n",
    "                           'batch_size': len(new_list),\n",
    "                           'n_channels': args['input_dimention'][-1],\n",
    "                           'norm_mode': args['normalization_mode']}  \n",
    "            test_set={}\n",
    "\n",
    "\n",
    "        \n",
    "        else:       \n",
    "            params_test = {'file_name': str(args['input_hdf5']), \n",
    "                           'dim': args['input_dimention'][0],\n",
    "                           'batch_size': len(new_list),\n",
    "                           'n_channels': args['input_dimention'][-1],\n",
    "                           'norm_mode': args['normalization_mode']}     \n",
    "    \n",
    "            test_generator = DataGeneratorTest(new_list, **params_test)\n",
    "            \n",
    "            if args['estimate_uncertainty']:\n",
    "                pred_PP = []\n",
    "                for mc in range(args['number_of_sampling']):\n",
    "                    predP = model.predict_generator(generator=test_generator)\n",
    "                    pred_PP.append(predP)\n",
    "                \n",
    "    \n",
    "                \n",
    "                pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list))\n",
    "                pred_SS_mean = pred_PP.mean(axis=0)\n",
    "                pred_SS_std = pred_PP.std(axis=0) \n",
    "\n",
    "            else:          \n",
    "                pred_SS_mean = model.predict_generator(generator=test_generator)\n",
    "                pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1]) \n",
    "                \n",
    "                pred_SS_std = np.zeros((pred_SS_mean.shape))   \n",
    "                \n",
    "   \n",
    "            test_set={}\n",
    "            fl = h5py.File(args['input_hdf5'], 'r')\n",
    "            for ID in new_list:\n",
    "                dataset = fl.get(str(ID))\n",
    "                test_set.update( {str(ID) : dataset})                 \n",
    "            \n",
    "            for ts in range(pred_SS_mean.shape[0]): \n",
    "                evi =  new_list[ts] \n",
    "                dataset = test_set[evi]  \n",
    "                \n",
    "                    \n",
    "                try:\n",
    "                    mag = float(dataset.attrs['magnitude']);\n",
    "                except Exception:     \n",
    "                    mag = None\n",
    "                \n",
    "                \n",
    "                #Ppick, perror, Pprob =  picker(args, pred_SS_mean[ts], pred_SS_std[ts], spt) \n",
    "                \n",
    "                #_output_writter_test(args, dataset, evi, test_writer, csvTst, Ppick, perror, Pprob)\n",
    "                        \n",
    "                pred_PPall.append(pred_PP[:,ts])\n",
    "                pred_SS_mean_all.append(pred_SS_mean[ts])\n",
    "                pred_SS_std_all.append(pred_SS_std[ts])\n",
    "                sptall.append(mag)\n",
    "                \n",
    "\n",
    "    np.save('pall_FoundationVit_MAg.npy',sptall)\n",
    "    np.save('pred_SS_all_FoundationVit_Mag.npy',pred_PPall)\n",
    "\n",
    "    end_training = time.time()  \n",
    "    delta = end_training - start_training\n",
    "    hour = int(delta / 3600)\n",
    "    delta -= hour * 3600\n",
    "    minute = int(delta / 60)\n",
    "    delta -= minute * 60\n",
    "    seconds = delta     \n",
    "                    \n",
    "    with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file: \n",
    "        the_file.write('================== Overal Info =============================='+'\\n')               \n",
    "                \n",
    "        the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\\n')            \n",
    "        the_file.write('input_testset: '+str(args['input_testset'])+'\\n')\n",
    "        the_file.write('input_model: '+str(args['input_model'])+'\\n')\n",
    "        the_file.write('output_name: '+str(args['output_name']+'_outputs')+'\\n')  \n",
    "        the_file.write('================== Testing Parameters ======================='+'\\n')  \n",
    "        the_file.write('mode: '+str(args['mode'])+'\\n')  \n",
    "        the_file.write('finished the test in:  {} hours and {} minutes and {} seconds \\n'.format(hour, minute, round(seconds, 3))) \n",
    "        the_file.write('batch_size: '+str(args['batch_size'])+'\\n')\n",
    "        the_file.write('total number of tests '+str(len(test))+'\\n')\n",
    "        the_file.write('gpuid: '+str(args['gpuid'])+'\\n')\n",
    "        the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\\n')             \n",
    "        the_file.write('================== Other Parameters ========================='+'\\n')            \n",
    "        the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\\n')\n",
    "        the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\\n')\n",
    "        the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\\n')                       \n",
    "        the_file.write('S_threshold: '+str(args['S_threshold'])+'\\n')\n",
    "        \n",
    "    \n",
    "    \n",
    "def _output_writter_test(args, \n",
    "                        dataset, \n",
    "                        evi, \n",
    "                        output_writer, \n",
    "                        csvfile, \n",
    "                        Ppick,\n",
    "                        perror,\n",
    "                        Pprob\n",
    "                        ):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Writes the detection & picking results into a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args: dic\n",
    "        A dictionary containing all of the input parameters.    \n",
    " \n",
    "    dataset: hdf5 obj\n",
    "        Dataset object of the trace.\n",
    "\n",
    "    evi: str\n",
    "        Trace name.    \n",
    "              \n",
    "    output_writer: obj\n",
    "        For writing out the detection/picking results in the CSV file.\n",
    "        \n",
    "    csvfile: obj\n",
    "        For writing out the detection/picking results in the CSV file.   \n",
    "             \n",
    "        \n",
    "    Returns\n",
    "    --------  \n",
    "    X_test_results.csv  \n",
    "    \n",
    "        \n",
    "    \"\"\"        \n",
    "    \n",
    "    \n",
    "    #print(dataset.attrs['data_category'] )\n",
    "    \n",
    "    if dataset.attrs['data_category'] != 'noise':                                     \n",
    "\n",
    "        s_arrival_sample = dataset.attrs['s_arrival_sample'] \n",
    "\n",
    "                   \n",
    "    elif dataset.attrs['data_category'] == 'noise':               \n",
    "        #network_code = dataset.attrs['network_code']\n",
    "        source_id = None\n",
    "        source_distance_km = None \n",
    "        snr_db = None\n",
    "        #trace_name = dataset.attrs['trace_name'] \n",
    "        #trace_category = dataset.attrs['trace_category']            \n",
    "        trace_start_time = None\n",
    "        source_magnitude = None\n",
    "        s_arrival_sample = None\n",
    "        S_status = None\n",
    "        S_weight = None\n",
    "        s_arrival_sample = None\n",
    "        s_status = None\n",
    "        s_weight = None\n",
    "        #receiver_type = dataset.attrs['receiver_type'] \n",
    "      \n",
    "    #print(Ppick[0])\n",
    "\n",
    "    output_writer.writerow([ \n",
    "\n",
    "                            s_arrival_sample, \n",
    "                             \n",
    "\n",
    "            \n",
    "                            Ppick[0], \n",
    "                            Pprob[0],\n",
    "                            perror[0],\n",
    "                            \n",
    "                            ]) \n",
    "    \n",
    "    csvfile.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester1(input_hdf5= '/scratch/sadalyom/DataCollected',\n",
    "       input_testset='test_Events.npy',\n",
    "       input_model = 'test_trainer_FoundationVit_Mag_outputs/models/test_trainer_FoundationVit_Mag_004.h5',\n",
    "       output_name='test_tester',\n",
    "       detection_threshold=0.1,                \n",
    "       p_threshold=0.1,\n",
    "       number_of_plots=3,\n",
    "       estimate_uncertainty=True, \n",
    "       number_of_sampling=1,\n",
    "       input_dimention=(6000, 3),\n",
    "       normalization_mode='std',\n",
    "       mode='generator',\n",
    "       batch_size=1024,\n",
    "       gpuid=None,\n",
    "       gpu_limit=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f7ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
