{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7660dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import warnings\n",
    "import contextlib\n",
    "import multiprocessing\n",
    "import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import signal\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, Flatten, Reshape, Activation,\n",
    "    Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization,\n",
    "    Add, concatenate, DepthwiseConv1D\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eedb6f",
   "metadata": {},
   "source": [
    "## Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_depth_rate = 0.1\n",
    "positional_emb = False\n",
    "conv_layers = 1\n",
    "num_classes = 1\n",
    "projection_dim = 80\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim, projection_dim]\n",
    "transformer_layers = 4\n",
    "\n",
    "\n",
    "def convF1(inpt, D1, fil_ord, Dr):\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    #filters = inpt._keras_shape[channel_axis]\n",
    "    filters = int(inpt.shape[-1])\n",
    "    \n",
    "    #infx = Activation(tf.nn.gelu')(inpt)\n",
    "    pre = Conv1D(filters,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(inpt)\n",
    "    pre = BatchNormalization()(pre)    \n",
    "    pre = Activation('linear')(pre)\n",
    "    \n",
    "    #shared_conv = Conv1D(D1,  fil_ord, strides =(1), padding='same')\n",
    "    \n",
    "    inf  = Conv1D(filters,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(pre)\n",
    "    inf = BatchNormalization()(inf)    \n",
    "    inf = Activation('linear')(inf)\n",
    "    inf = Add()([inf,inpt])\n",
    "    \n",
    "    inf1  = Conv1D(D1,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(inf)\n",
    "    inf1 = BatchNormalization()(inf1)  \n",
    "    inf1 = Activation('linear')(inf1)    \n",
    "    encode = Dropout(Dr)(inf1, training=False)\n",
    "\n",
    "    return encode\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n",
    "\n",
    "\n",
    "class CCTTokenizer1(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=(1,1,1,1),\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[int(projection_dim)] * 8,\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer1, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_model = tf.keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv1D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        reshaped = tf.reshape(outputs, (-1, tf.shape(outputs)[1], tf.shape(outputs)[2]))\n",
    "        return outputs\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, 1))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = int(dummy_outputs.shape[1])\n",
    "            projection_dim = int(dummy_outputs.shape[-1])\n",
    "\n",
    "            embed_layer = layers.Embedding(input_dim=sequence_length, output_dim=projection_dim)\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def create_cct_model1(inputs):\n",
    "    cct_tokenizer = CCTTokenizer1()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    return representation\n",
    "\n",
    "\n",
    "def UNET(inputs, D1):\n",
    "    D2 = int(D1 * 2)\n",
    "    D3 = int(D2 * 2)\n",
    "    D4 = int(D3 * 2)\n",
    "    D5 = int(D4 * 2)\n",
    "\n",
    "    conv1 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=(2))(conv1)\n",
    "\n",
    "    conv2 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=(2))(conv2)\n",
    "\n",
    "    conv3 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=(2))(conv3)\n",
    "\n",
    "    conv4 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=(2))(conv4)\n",
    "\n",
    "    conv44 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv44 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv44)\n",
    "    pool44 = MaxPooling1D(pool_size=(5))(conv44)\n",
    "\n",
    "    conv5 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool44)\n",
    "    conv5 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    drop5 = create_cct_model1(conv5)\n",
    "\n",
    "    up66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(5))(drop5))\n",
    "    merge66 = concatenate([pool4, up66], axis=-1)\n",
    "    conv66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge66)\n",
    "    conv66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv66)\n",
    "\n",
    "    up6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv66))\n",
    "    merge6 = concatenate([conv4, up6], axis=-1)\n",
    "    conv6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv1D(D3, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=-1)\n",
    "    conv7 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv1D(D2, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=-1)\n",
    "    conv8 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv1D(D1, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=-1)\n",
    "    conv9 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    return conv9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec1fff",
   "metadata": {},
   "source": [
    "## CCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4540780",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (6000,3)\n",
    "\n",
    "def mlpx(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        #x = layers.Dense(units, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "    \n",
    "drop_ratex = 0.2\n",
    "stochastic_depth_ratex = 0.1\n",
    "\n",
    "w1x = 100\n",
    "\n",
    "positional_embx= False\n",
    "conv_layersx = 2\n",
    "num_classesx = 1\n",
    "input_shapex = (6000,3)\n",
    "image_sizex = 6000  # We'll resize input images to this size\n",
    "projection_dimx = int(2*w1x)\n",
    "num_headsx = 4\n",
    "transformer_unitsx = [\n",
    "    projection_dimx,\n",
    "    projection_dimx,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layersx = 4\n",
    "\n",
    "\n",
    "class CCTTokenizer1x(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=4,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=(2,2,2,2,2,2,2,2),\n",
    "        num_conv_layersx=conv_layersx,\n",
    "        num_output_channels=[int(projection_dimx), int(projection_dimx), int(projection_dimx), int(projection_dimx), int(projection_dimx), int(projection_dimx), int(projection_dimx), int(projection_dimx)],\n",
    "        positional_embx=positional_embx,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer1x, self).__init__(**kwargs)\n",
    "\n",
    "        # This is our tokenizer.\n",
    "        self.conv_model = tf.keras.Sequential()\n",
    "        for i in range(num_conv_layersx):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv1D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "            #self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            self.conv_model.add(\n",
    "                layers.MaxPool1D(pooling_kernel_size, (pooling_stride[i]), \"same\")\n",
    "            )\n",
    "\n",
    "        self.positional_embx= positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        # After passing the images through our mini-network the spatial dimensions\n",
    "        # are flattened to form sequences.\n",
    "        reshaped = tf.reshape(\n",
    "            outputs,\n",
    "            (-1, tf.shape(outputs)[1], tf.shape(outputs)[-1]),\n",
    "        )\n",
    "        return reshaped\n",
    "\n",
    "    def positional_embeddingx(self, image_sizex):\n",
    "        # Positional embeddings are optional in CCT. Here, we calculate\n",
    "        # the number of sequences and initialize an `Embedding` layer to\n",
    "        # compute the positional embeddings later.\n",
    "        if self.positional_embx:\n",
    "            dummy_inputs = tf.ones((1, image_sizex, 1))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = dummy_outputs.shape[1]\n",
    "            projection_dimx = dummy_outputs.shape[-1]\n",
    "\n",
    "            print(dummy_outputs,sequence_length,projection_dimx)\n",
    "            embed_layer = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=projection_dimx\n",
    "            )\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "# Referred from: github.com:rwightman/pytorch-image-models.\n",
    "class StochasticDepthx(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepthx, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n",
    "          \n",
    "def create_cct_model1x(inputs):\n",
    "\n",
    "\n",
    "    # Augment data.\n",
    "    #augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Encode patches.\n",
    "    cct_tokenizer = CCTTokenizer1x()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    # Apply positional embedding.\n",
    "    if positional_embx:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embeddingx(image_sizex)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_ratex, transformer_layersx)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layersx):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_headsx, key_dim=projection_dimx, dropout=0.2\n",
    "        )(x1, x1)\n",
    "\n",
    "        #print(encoded_patches)\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepthx(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        #x3 = x2\n",
    "        \n",
    "        # mlpx.\n",
    "        x3 = mlpx(x3, hidden_units=transformer_unitsx, dropout_rate=0.2)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        #print(x3)\n",
    "        x3 = StochasticDepthx(dpr[i])(x3)\n",
    "        #print(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "     \n",
    "    # Apply sequence pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    \n",
    "    ''' \n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "    '''\n",
    "    return representation\n",
    "\n",
    "\n",
    "def construct_model(inputs,inp):\n",
    "    \n",
    "    x = convF1(inputs,   80, 13, 0.1)\n",
    "    x = convF1(x,   80, 13, 0.1)\n",
    "    x = convF1(x,   80, 13, 0.1)\n",
    "    x = Flatten()(x)\n",
    "    x =  Reshape((6000,1))(x)\n",
    "    x = concatenate([x,inp])\n",
    "\n",
    "    featuresP = create_cct_model1x(x)\n",
    "    featuresP = layers.Flatten()(featuresP)\n",
    "    featuresP = layers.Dropout(0.2)(featuresP)\n",
    "    #logitp = layers.Dense(1, activation='sigmoid')(featuresP)\n",
    "\n",
    "\n",
    "    #logitp  = Conv2D(1,  3, strides =(1), padding='same',activation='sigmoid', kernel_initializer='he_normal',name='picker_P')(featuresP)\n",
    "\n",
    "\n",
    "    #model = Model(inputs=[inputs], outputs=[logitp])\n",
    "#     model.summary()\n",
    "    \n",
    "    return featuresP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53fe5a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d835d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from EqT_utils_Pol import DataGeneratorTest\n",
    "\n",
    "\n",
    "def generate_arrays_from_file(file_list, step):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Make a generator to generate list of trace names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_list : str\n",
    "        A list of trace names.  \n",
    "        \n",
    "    step : int\n",
    "        Batch size.  \n",
    "        \n",
    "    Returns\n",
    "    --------  \n",
    "    chunck : str\n",
    "        A batch of trace names. \n",
    "        \n",
    "    \"\"\"     \n",
    "    \n",
    "    n_loops = int(np.ceil(len(file_list) / step))\n",
    "    b = 0\n",
    "    while True:\n",
    "        for i in range(n_loops):\n",
    "            e = i*step + step \n",
    "            if e > len(file_list):\n",
    "                e = len(file_list)\n",
    "            chunck = file_list[b:e]\n",
    "            b=e\n",
    "            yield chunck   \n",
    "\n",
    "def tester1(input_hdf5=None,\n",
    "           input_testset=None,\n",
    "           input_model=None,\n",
    "           output_name=None,\n",
    "           detection_threshold=0.20,                \n",
    "           p_threshold=0.1,\n",
    "           S_threshold=0.1, \n",
    "           number_of_plots=100,\n",
    "           estimate_uncertainty=True, \n",
    "           number_of_sampling=5,\n",
    "           loss_weights=[0.05, 0.40, 0.55],\n",
    "           loss_types=['mse'],\n",
    "           input_dimention=(6000, 3),\n",
    "           normalization_mode='std',\n",
    "           mode='generator',\n",
    "           batch_size=500,\n",
    "           gpuid=None,\n",
    "           gpu_limit=None):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Applies a trained model to a windowed waveform to perform both detection and picking at the same time.  \n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_hdf5: str, default=None\n",
    "        Path to an hdf5 file containing only one class of \"data\" with NumPy arrays containing 3 component waveforms each 1 min long.\n",
    "\n",
    "    input_testset: npy, default=None\n",
    "        Path to a NumPy file (automaticaly generated by the trainer) containing a list of trace names.        \n",
    "\n",
    "    input_model: str, default=None\n",
    "        Path to a trained model.\n",
    "        \n",
    "    output_dir: str, default=None\n",
    "        Output directory that will be generated. \n",
    "        \n",
    "    output_probabilities: bool, default=False\n",
    "        If True, it will output probabilities and estimated uncertainties for each trace into an HDF file. \n",
    "       \n",
    "          \n",
    "    S_threshold: float, default=0.1\n",
    "        A value which the P probabilities above it will be considered as P arrival.\n",
    "\n",
    "               \n",
    "    number_of_plots: float, default=10\n",
    "        The number of plots for detected events outputed for each station data.\n",
    "        \n",
    "    estimate_uncertainty: bool, default=False\n",
    "        If True uncertainties in the output probabilities will be estimated.  \n",
    "        \n",
    "    number_of_sampling: int, default=5\n",
    "        Number of sampling for the uncertainty estimation. \n",
    "               \n",
    "             \n",
    "    input_dimention: tuple, default=(6000, 3)\n",
    "        Loss types for P picking.          \n",
    "\n",
    "    normalization_mode: str, default='std' \n",
    "        Mode of normalization for data preprocessing, 'max', maximum amplitude among three components, 'std', standard deviation.\n",
    "\n",
    "    mode: str, default='generator'\n",
    "        Mode of running. 'pre_load_generator' or 'generator'.\n",
    "                      \n",
    "    batch_size: int, default=500 \n",
    "        Batch size. This wont affect the speed much but can affect the performance.\n",
    "\n",
    "    gpuid: int, default=None\n",
    "        Id of GPU used for the prediction. If using CPU set to None.\n",
    "         \n",
    "    gpu_limit: int, default=None\n",
    "        Set the maximum percentage of memory usage for the GPU.\n",
    "        \n",
    "\n",
    "    \"\"\" \n",
    "              \n",
    "         \n",
    "    args = {\n",
    "    \"input_hdf5\": input_hdf5,\n",
    "    \"input_testset\": input_testset,\n",
    "    \"input_model\": input_model,\n",
    "    \"output_name\": output_name,\n",
    "    \"detection_threshold\": detection_threshold,\n",
    "    \"S_threshold\": S_threshold,\n",
    "    \"number_of_plots\": number_of_plots,\n",
    "    \"estimate_uncertainty\": estimate_uncertainty,\n",
    "    \"number_of_sampling\": number_of_sampling,\n",
    "    \"input_dimention\": input_dimention,\n",
    "    \"normalization_mode\": normalization_mode,\n",
    "    \"mode\": mode,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"gpuid\": gpuid,\n",
    "    \"gpu_limit\": gpu_limit\n",
    "    }  \n",
    "\n",
    "    \n",
    "    if args['gpuid']:           \n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])\n",
    "        tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit']) \n",
    "        K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "    \n",
    "    save_dir = os.path.join(os.getcwd(), str(args['output_name'])+'_outputs')\n",
    "    save_figs = os.path.join(save_dir, 'figures')\n",
    " \n",
    "    if os.path.isdir(save_dir):\n",
    "        shutil.rmtree(save_dir)  \n",
    "    os.makedirs(save_figs) \n",
    " \n",
    "    test = np.load(args['input_testset'])\n",
    "\n",
    "    print('Loading the model ...', flush=True) \n",
    "\n",
    "\n",
    "    \n",
    "    D1 = 5\n",
    "    D2 = int(D1*2)\n",
    "    D3 = int(D2*2)\n",
    "    D4 = int(D3*2)\n",
    "    D5 = int(D4*2)\n",
    "\n",
    "    inp = Input(shape=input_shape,name=\"input\")\n",
    "    conv1 = UNET(inp,D1)\n",
    "    out = Conv1D(D1,  3, strides =(1), padding='same',kernel_initializer='he_normal')(conv1)\n",
    "    out = Conv1D(3,  3, strides =(1), padding='same',kernel_initializer='he_normal',name='picker_PP')(out)\n",
    "    modeloriginal = Model(inp, out)\n",
    "\n",
    "    modeloriginal.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])     \n",
    "    modeloriginal.load_weights('../../../weights/UTrans_Foundation.h5')\n",
    "\n",
    "    # Model CCT\n",
    "    inputs = modeloriginal.layers[63].output  # layer that you want to connect your new FC layer to \n",
    "\n",
    "\n",
    "    features = construct_model(inputs,inp)\n",
    "    #features = Reshape((6000,1))(features)\n",
    "\n",
    "    #e = Dense(1)(features)\n",
    "    o = layers.Dense(1, activation='sigmoid',name='output_layer')(features)\n",
    "    model = Model(inputs=modeloriginal.input, outputs=o)\n",
    "\n",
    "\n",
    "    Adm = tensorflow.optimizers.Adam(lr=1e-4)\n",
    "    model.compile(optimizer=Adm,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])      \n",
    "        \n",
    "\n",
    "    model.load_weights(args['input_model'])\n",
    "\n",
    "\n",
    "    model.summary()  \n",
    "        \n",
    "    print('Loading is complete!', flush=True)  \n",
    "    print('Testing ...', flush=True)    \n",
    "    print('Writting results into: \" ' + str(args['output_name'])+'_outputs'+' \"', flush=True)\n",
    "    \n",
    "    start_training = time.time()          \n",
    "\n",
    "    csvTst = open(os.path.join(save_dir,'X_test_results.csv'), 'w')          \n",
    "    test_writer = csv.writer(csvTst, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    test_writer.writerow([\n",
    "                          \n",
    "                          \n",
    "                          's_arrival_sample', \n",
    "                          \n",
    "\n",
    "                          \n",
    "                          'S_pick',\n",
    "                          'S_probability',\n",
    "                          'S_error'\n",
    "                          ])  \n",
    "    csvTst.flush()        \n",
    "        \n",
    "    plt_n = 0\n",
    "    list_generator = generate_arrays_from_file(test, args['batch_size']) \n",
    "    pred_SS_mean_all=[]\n",
    "    pred_SS_std_all=[]\n",
    "    sptall = []\n",
    "    pred_PPall = []\n",
    "    \n",
    "    pbar_test = tqdm(total= int(np.ceil(len(test)/args['batch_size'])))            \n",
    "    for _ in range(int(np.ceil(len(test) / args['batch_size']))):\n",
    "        pbar_test.update()\n",
    "        new_list = next(list_generator)\n",
    "\n",
    "        if args['mode'].lower() == 'pre_load_generator':                \n",
    "            params_test = {'dim': args['input_dimention'][0],\n",
    "                           'batch_size': len(new_list),\n",
    "                           'n_channels': args['input_dimention'][-1],\n",
    "                           'norm_mode': args['normalization_mode']}  \n",
    "            test_set={}\n",
    "\n",
    "\n",
    "        \n",
    "        else:       \n",
    "            params_test = {'file_name': str(args['input_hdf5']), \n",
    "                           'dim': args['input_dimention'][0],\n",
    "                           'batch_size': len(new_list),\n",
    "                           'n_channels': args['input_dimention'][-1],\n",
    "                           'norm_mode': args['normalization_mode']}     \n",
    "    \n",
    "            test_generator = DataGeneratorTest(new_list, **params_test)\n",
    "            \n",
    "            if args['estimate_uncertainty']:\n",
    "                pred_PP = []\n",
    "                for mc in range(args['number_of_sampling']):\n",
    "                    predP = model.predict_generator(generator=test_generator)\n",
    "                    pred_PP.append(predP)\n",
    "                \n",
    "    \n",
    "                \n",
    "                pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list))\n",
    "                pred_SS_mean = pred_PP.mean(axis=0)\n",
    "                pred_SS_std = pred_PP.std(axis=0) \n",
    "\n",
    "            else:          \n",
    "                pred_SS_mean = model.predict_generator(generator=test_generator)\n",
    "                pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1]) \n",
    "                \n",
    "                pred_SS_std = np.zeros((pred_SS_mean.shape))   \n",
    "                \n",
    "   \n",
    "            test_set={}\n",
    "            fl = h5py.File(args['input_hdf5'], 'r')\n",
    "            for ID in new_list:\n",
    "                dataset = fl.get(str(ID))\n",
    "                test_set.update( {str(ID) : dataset})                 \n",
    "            \n",
    "            for ts in range(pred_SS_mean.shape[0]): \n",
    "                evi =  new_list[ts] \n",
    "                dataset = test_set[evi]  \n",
    "                \n",
    "                    \n",
    "                try:\n",
    "                    mag = dataset.attrs['P_polarity'];\n",
    "                except Exception:     \n",
    "                    mag = None\n",
    "                \n",
    "                pred_PPall.append(pred_PP[:,ts])\n",
    "                pred_SS_mean_all.append(pred_SS_mean[ts])\n",
    "                pred_SS_std_all.append(pred_SS_std[ts])\n",
    "                sptall.append(mag)\n",
    "                \n",
    "\n",
    "    np.save('pall_FoundationVit_Pol.npy',sptall)\n",
    "    np.save('pred_SS_all_FoundationVit_Pol.npy',pred_PPall)\n",
    "\n",
    "    end_training = time.time()  \n",
    "    delta = end_training - start_training\n",
    "    hour = int(delta / 3600)\n",
    "    delta -= hour * 3600\n",
    "    minute = int(delta / 60)\n",
    "    delta -= minute * 60\n",
    "    seconds = delta     \n",
    "                    \n",
    "    with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file: \n",
    "        the_file.write('================== Overal Info =============================='+'\\n')               \n",
    "                \n",
    "        the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\\n')            \n",
    "        the_file.write('input_testset: '+str(args['input_testset'])+'\\n')\n",
    "        the_file.write('input_model: '+str(args['input_model'])+'\\n')\n",
    "        the_file.write('output_name: '+str(args['output_name']+'_outputs')+'\\n')  \n",
    "        the_file.write('================== Testing Parameters ======================='+'\\n')  \n",
    "        the_file.write('mode: '+str(args['mode'])+'\\n')  \n",
    "        the_file.write('finished the test in:  {} hours and {} minutes and {} seconds \\n'.format(hour, minute, round(seconds, 3))) \n",
    "        the_file.write('batch_size: '+str(args['batch_size'])+'\\n')\n",
    "        the_file.write('total number of tests '+str(len(test))+'\\n')\n",
    "        the_file.write('gpuid: '+str(args['gpuid'])+'\\n')\n",
    "        the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\\n')             \n",
    "        the_file.write('================== Other Parameters ========================='+'\\n')            \n",
    "        the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\\n')\n",
    "        the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\\n')\n",
    "        the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\\n')                       \n",
    "        the_file.write('S_threshold: '+str(args['S_threshold'])+'\\n')\n",
    "        \n",
    "    \n",
    "    \n",
    "def _output_writter_test(args, \n",
    "                        dataset, \n",
    "                        evi, \n",
    "                        output_writer, \n",
    "                        csvfile, \n",
    "                        Ppick,\n",
    "                        perror,\n",
    "                        Pprob\n",
    "                        ):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Writes the detection & picking results into a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args: dic\n",
    "        A dictionary containing all of the input parameters.    \n",
    " \n",
    "    dataset: hdf5 obj\n",
    "        Dataset object of the trace.\n",
    "\n",
    "    evi: str\n",
    "        Trace name.    \n",
    "              \n",
    "    output_writer: obj\n",
    "        For writing out the detection/picking results in the CSV file.\n",
    "        \n",
    "    csvfile: obj\n",
    "        For writing out the detection/picking results in the CSV file.   \n",
    "             \n",
    "        \n",
    "    Returns\n",
    "    --------  \n",
    "    X_test_results.csv  \n",
    "    \n",
    "        \n",
    "    \"\"\"        \n",
    "    \n",
    "    \n",
    "    #print(dataset.attrs['data_category'] )\n",
    "    \n",
    "    if dataset.attrs['data_category'] != 'noise':                                     \n",
    "\n",
    "        s_arrival_sample = dataset.attrs['s_arrival_sample'] \n",
    "\n",
    "                   \n",
    "    elif dataset.attrs['data_category'] == 'noise':               \n",
    "        #network_code = dataset.attrs['network_code']\n",
    "        source_id = None\n",
    "        source_distance_km = None \n",
    "        snr_db = None\n",
    "        #trace_name = dataset.attrs['trace_name'] \n",
    "        #trace_category = dataset.attrs['trace_category']            \n",
    "        trace_start_time = None\n",
    "        source_magnitude = None\n",
    "        s_arrival_sample = None\n",
    "        S_status = None\n",
    "        S_weight = None\n",
    "        s_arrival_sample = None\n",
    "        s_status = None\n",
    "        s_weight = None\n",
    "        #receiver_type = dataset.attrs['receiver_type'] \n",
    "      \n",
    "    #print(Ppick[0])\n",
    "\n",
    "    output_writer.writerow([ \n",
    "\n",
    "                            s_arrival_sample, \n",
    "                             \n",
    "\n",
    "            \n",
    "                            Ppick[0], \n",
    "                            Pprob[0],\n",
    "                            perror[0],\n",
    "                            \n",
    "                            ]) \n",
    "    \n",
    "    csvfile.flush()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#`input_model: Path to the trained model directory: Change based on the best model you obtained\n",
    "#`input_hdf5`: Path to the full waveform HDF5 file.\n",
    "\n",
    "tester1(input_hdf5= '/scratch/sadalyom/DataCollected',\n",
    "       input_testset='test_Events_Pol.npy',\n",
    "       input_model = 'test_trainer_FoundationVit_Pol_outputs/models/test_trainer_FoundationVit_Pol_025.h5',\n",
    "       output_name='test_tester',\n",
    "       detection_threshold=0.1,                \n",
    "       p_threshold=0.1,\n",
    "       number_of_plots=3,\n",
    "       estimate_uncertainty=True, \n",
    "       number_of_sampling=1,\n",
    "       input_dimention=(6000, 3),\n",
    "       normalization_mode='std',\n",
    "       mode='generator',\n",
    "       batch_size=128,\n",
    "       gpuid=None,\n",
    "       gpu_limit=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f7ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
