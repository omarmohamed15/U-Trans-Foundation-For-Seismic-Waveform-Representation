{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934eecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "tf.config.experimental.set_visible_devices(physical_devices[3], 'GPU')\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e78a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import warnings\n",
    "import contextlib\n",
    "import multiprocessing\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import signal\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, Flatten, Reshape, Activation,\n",
    "    Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization,\n",
    "    Add, concatenate\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import faulthandler\n",
    "faulthandler.enable()\n",
    "\n",
    "# External utilities (as in your original code)\n",
    "from EQCCT_P_utils import DataGenerator, _lr_schedule, data_reader\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "def wbceEdit(y_true, y_pred):\n",
    "    ms = K.mean(K.square(y_true - y_pred))\n",
    "    ssim = 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n",
    "    return (ssim + ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16e92e",
   "metadata": {},
   "source": [
    "## Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f81313",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_depth_rate = 0.1\n",
    "positional_emb = False\n",
    "conv_layers = 1\n",
    "num_classes = 1\n",
    "\n",
    "input_shape = (375, 256)\n",
    "projection_dim = 80\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim, projection_dim]\n",
    "transformer_layers = 4\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n",
    "\n",
    "\n",
    "class CCTTokenizer1(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=(1,1,1,1),\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[int(projection_dim)] * 8,\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer1, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_model = tf.keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv1D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        reshaped = tf.reshape(outputs, (-1, tf.shape(outputs)[1], tf.shape(outputs)[2]))\n",
    "        return outputs\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, 1))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = int(dummy_outputs.shape[1])\n",
    "            projection_dim = int(dummy_outputs.shape[-1])\n",
    "\n",
    "            embed_layer = layers.Embedding(input_dim=sequence_length, output_dim=projection_dim)\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def create_cct_model1(inputs):\n",
    "    cct_tokenizer = CCTTokenizer1()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    return representation\n",
    "\n",
    "\n",
    "def UNET(inputs, D1):\n",
    "    D2 = int(D1 * 2)\n",
    "    D3 = int(D2 * 2)\n",
    "    D4 = int(D3 * 2)\n",
    "    D5 = int(D4 * 2)\n",
    "\n",
    "    conv1 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=(2))(conv1)\n",
    "\n",
    "    conv2 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=(2))(conv2)\n",
    "\n",
    "    conv3 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=(2))(conv3)\n",
    "\n",
    "    conv4 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=(2))(conv4)\n",
    "\n",
    "    conv44 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv44 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv44)\n",
    "    pool44 = MaxPooling1D(pool_size=(5))(conv44)\n",
    "\n",
    "    conv5 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool44)\n",
    "    conv5 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    drop5 = create_cct_model1(conv5)\n",
    "\n",
    "    up66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(5))(drop5))\n",
    "    merge66 = concatenate([pool4, up66], axis=-1)\n",
    "    conv66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge66)\n",
    "    conv66 = Conv1D(D5, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv66)\n",
    "\n",
    "    up6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv66))\n",
    "    merge6 = concatenate([conv4, up6], axis=-1)\n",
    "    conv6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv1D(D4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv1D(D3, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=-1)\n",
    "    conv7 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv1D(D3, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv1D(D2, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=-1)\n",
    "    conv8 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv1D(D2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv1D(D1, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling1D(size=(2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=-1)\n",
    "    conv9 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv1D(D1, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    return conv9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601f6c3",
   "metadata": {},
   "source": [
    "## EQCCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f276de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "    \n",
    "def f1(y_true, y_pred):\n",
    "           \n",
    "    precisionx = precision(y_true, y_pred)\n",
    "    recallx = recall(y_true, y_pred)\n",
    "    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "def wbceEdit( y_true, y_pred) :\n",
    "    \n",
    "    ms = K.mean(K.square(y_true-y_pred)) \n",
    "    \n",
    "    ssim = 1-tf.reduce_mean(tf.image.ssim(y_true,y_pred,1.0))\n",
    "    \n",
    "    return (ssim + ms)\n",
    "\n",
    "\n",
    "stochastic_depth_rateX = 0.1\n",
    "positional_embX = False\n",
    "input_shapeX = (6000, 3)\n",
    "image_sizeX = 6000  # We'll resize input images to this size\n",
    "patch_sizeX = 40  # Size of the patches to be extract from the input images\n",
    "num_patchesX = (image_sizeX // patch_sizeX)\n",
    "projection_dimX = 40\n",
    "\n",
    "num_headsX = 4\n",
    "transformer_unitsX = [\n",
    "    projection_dimX,\n",
    "    projection_dimX,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layersX = 4\n",
    "\n",
    "    \n",
    "class PatchEncoderX(layers.Layer):\n",
    "    def __init__(self, num_patchesX, projection_dimX, **kwargs):\n",
    "        super(PatchEncoderX, self).__init__()\n",
    "        self.num_patchesX = num_patchesX\n",
    "        self.projection = layers.Dense(units=projection_dimX)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patchesX, output_dim=projection_dimX\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_patchesX' : self.num_patchesX, \n",
    "            'projection_dimX' : projection_dimX, \n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patchesX, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        \n",
    "        \n",
    "        return encoded\n",
    "    \n",
    "# Referred from: github.com:rwightman/pytorch-image-models.\n",
    "class StochasticDepthX(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepthX, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n",
    "\n",
    "def convF1(inpt, D1, fil_ord, Dr):\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    #filters = inpt._keras_shape[channel_axis]\n",
    "    filters = int(inpt.shape[-1])\n",
    "    \n",
    "    pre = Conv1D(filters,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(inpt)\n",
    "    pre = BatchNormalization()(pre)    \n",
    "    pre = Activation(tf.nn.gelu)(pre)\n",
    "    \n",
    "    \n",
    "    inf  = Conv1D(filters,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(pre)\n",
    "    inf = BatchNormalization()(inf)    \n",
    "    inf = Activation(tf.nn.gelu)(inf)\n",
    "    inf = Add()([inf,inpt])\n",
    "    \n",
    "    inf1  = Conv1D(D1,  fil_ord, strides =(1), padding='same',kernel_initializer='he_normal')(inf)\n",
    "    inf1 = BatchNormalization()(inf1)  \n",
    "    inf1 = Activation(tf.nn.gelu)(inf1)    \n",
    "    encode = Dropout(Dr)(inf1)\n",
    "\n",
    "    return encode\n",
    "\n",
    "def mlpX(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        #x = layers.Dense(units, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class PatchesX(layers.Layer):\n",
    "    def __init__(self, patch_sizeX, **kwargs):\n",
    "        super(PatchesX, self).__init__()\n",
    "        self.patch_sizeX = patch_sizeX\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'patch_sizeX' : self.patch_sizeX, \n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config\n",
    "        \n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_sizeX, 1, 1],\n",
    "            strides=[1, self.patch_sizeX, 1, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "class PatchEncoderX(layers.Layer):\n",
    "    def __init__(self, num_patchesX, projection_dimX, **kwargs):\n",
    "        super(PatchEncoderX, self).__init__()\n",
    "        self.num_patchesX = num_patchesX\n",
    "        self.projection = layers.Dense(units=projection_dimX)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patchesX, output_dim=projection_dimX\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_patchesX' : self.num_patchesX, \n",
    "            'projection_dimX' : projection_dimX, \n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patchesX, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "    \n",
    "        \n",
    "        return encoded\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def create_cct_modelP(inputs,inp):\n",
    "    \n",
    "    ## Decoder of the Foundation Encoder\n",
    "    x = convF1(inputs,   80, 13, 0.1)\n",
    "    x = convF1(x,   80, 13, 0.1)\n",
    "    x = convF1(x,   80, 13, 0.1)\n",
    "    x = Flatten()(x)\n",
    "    x =  Reshape((6000,1))(x)\n",
    "    ## Ready to Concatenate\n",
    "    x = concatenate([x,inp])\n",
    "    \n",
    "    \n",
    "    inputs1 = convF1(x,   10, 11, 0.1)\n",
    "    inputs1 = convF1(inputs1, 20, 11, 0.1)\n",
    "    inputs1 = convF1(inputs1, 40, 11, 0.1)\n",
    "    \n",
    "    inputreshaped = layers.Reshape((6000,1,40))(inputs1)\n",
    "    # Create patches.\n",
    "    patchesX = PatchesX(patch_sizeX)(inputreshaped)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoderX(num_patchesX, projection_dimX)(patchesX)\n",
    "\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rateX, transformer_layersX)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layersX):\n",
    "        #encoded_patches = convF1(encoded_patches, 80,3, 0.1)\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_headsX, key_dim=projection_dimX, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        #attention_output = convF1(attention_output, 80,3, 0.1)\n",
    "        \n",
    "\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepthX(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlpX(x3, hidden_units=transformer_unitsX, dropout_rate=0.1)\n",
    "        #print(x3,x2)\n",
    "        \n",
    "        # Skip connection 2.\n",
    "        x3 = StochasticDepthX(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    \n",
    "    return representation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11044b1f",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce094e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import multiprocessing\n",
    "from EQCCT_P_utils import DataGenerator, _lr_schedule, data_reader\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "\n",
    "def picker(args, yh3, yh3_std, spt=None):\n",
    "    \"\"\" \n",
    "    \n",
    "    Performs detection and picking.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : dic\n",
    "        A dictionary containing all of the input parameters.  \n",
    "        \n",
    "        \n",
    "    yh3 : 1D array\n",
    "        P arrival probabilities.  \n",
    "        \n",
    "    yh3_std : 1D array\n",
    "        P arrival standard deviations.  \n",
    "        \n",
    "        \n",
    "    spt : {int, None}, default=None    \n",
    "        P arrival time in sample.\n",
    "        \n",
    "        \n",
    "    \"\"\"               \n",
    "        \n",
    "\n",
    "\n",
    "    P_PICKall=[]\n",
    "    Ppickall=[]\n",
    "    Pproball = []\n",
    "    perrorall=[]\n",
    "\n",
    "\n",
    "\n",
    "    sP_arr = _detect_peaks(yh3, mph=args['P_threshold'], mpd=1)\n",
    "\n",
    "    P_PICKS = []\n",
    "    pick_errors = []\n",
    "    if len(sP_arr) > 0:\n",
    "        P_uncertainty = None  \n",
    "\n",
    "        for pick in range(len(sP_arr)):        \n",
    "            sauto = sP_arr[pick]\n",
    "\n",
    "            if  args['estimate_uncertainty'] and sauto:\n",
    "                P_uncertainty = np.round(yh3_std[int(sauto)], 3)\n",
    "\n",
    "            if sauto: \n",
    "                P_prob = np.round(yh3[int(sauto)], 3) \n",
    "                P_PICKS.append([sauto,P_prob, P_uncertainty]) \n",
    "\n",
    "    so=[]\n",
    "    si=[]\n",
    "    P_PICKS = np.array(P_PICKS)\n",
    "    P_PICKall.append(P_PICKS)\n",
    "    for ij in P_PICKS:\n",
    "        so.append(ij[1])\n",
    "        si.append(ij[0])\n",
    "    try:\n",
    "        so = np.array(so)\n",
    "        inds = np.argmax(so)\n",
    "        swave = si[inds]\n",
    "        perrorall.append(int(spt- swave))  \n",
    "        Ppickall.append(int(swave))\n",
    "        Pproball.append(int(np.max(so)))\n",
    "    except:\n",
    "        perrorall.append(None)\n",
    "        Ppickall.append(None)\n",
    "        Pproball.append(None)\n",
    "\n",
    "\n",
    "    #Ppickall = np.array(Ppickall)\n",
    "    #perrorall = np.array(perrorall)  \n",
    "    #Pproball = np.array(Pproball)\n",
    "    \n",
    "    return Ppickall, perrorall, Pproball\n",
    "\n",
    "def _detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising', kpsh=False, valley=False):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Detect peaks in data based on their amplitude and other features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1D array_like\n",
    "        data.\n",
    "        \n",
    "    mph : {None, number}, default=None\n",
    "        detect peaks that are greater than minimum peak height.\n",
    "        \n",
    "    mpd : int, default=1\n",
    "        detect peaks that are at least separated by minimum peak distance (in number of data).\n",
    "        \n",
    "    threshold : int, default=0\n",
    "        detect peaks (valleys) that are greater (smaller) than `threshold in relation to their immediate neighbors.\n",
    "        \n",
    "    edge : str, default=rising\n",
    "        for a flat peak, keep only the rising edge ('rising'), only the falling edge ('falling'), both edges ('both'), or don't detect a flat peak (None).\n",
    "        \n",
    "    kpsh : bool, default=False\n",
    "        keep peaks with same height even if they are closer than `mpd`.\n",
    "        \n",
    "    valley : bool, default=False\n",
    "        if True (1), detect valleys (local minima) instead of peaks.\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    ind : 1D array_like\n",
    "        indeces of the peaks in `x`.\n",
    "\n",
    "    Modified from \n",
    "   ----------------\n",
    "    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.atleast_1d(x).astype('float64')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    # handle NaN's\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    # handle NaN's\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size-1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                    & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    return ind\n",
    "\n",
    "\n",
    "def generate_arrays_from_file(file_list, step):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Make a generator to generate list of trace names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_list : str\n",
    "        A list of trace names.  \n",
    "        \n",
    "    step : int\n",
    "        Batch size.  \n",
    "        \n",
    "    Returns\n",
    "    --------  \n",
    "    chunck : str\n",
    "        A batch of trace names. \n",
    "        \n",
    "    \"\"\"     \n",
    "    \n",
    "    n_loops = int(np.ceil(len(file_list) / step))\n",
    "    b = 0\n",
    "    while True:\n",
    "        for i in range(n_loops):\n",
    "            e = i*step + step \n",
    "            if e > len(file_list):\n",
    "                e = len(file_list)\n",
    "            chunck = file_list[b:e]\n",
    "            b=e\n",
    "            yield chunck   \n",
    "class DataGeneratorTest(keras.utils.Sequence):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Keras generator with preprocessing. For testing. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list_IDsx: str\n",
    "        List of trace names.\n",
    "            \n",
    "    file_name: str\n",
    "        Path to the input hdf5 file.\n",
    "            \n",
    "    dim: tuple\n",
    "        Dimension of input traces. \n",
    "           \n",
    "    batch_size: int, default=32\n",
    "        Batch size.\n",
    "            \n",
    "    n_channels: int, default=3\n",
    "        Number of channels.\n",
    "            \n",
    "    norm_mode: str, default=max\n",
    "        The mode of normalization, 'max' or 'std'.\n",
    "            \n",
    "    Returns\n",
    "    --------        \n",
    "    Batches of two dictionaries: {'input': X}: pre-processed waveform as input {'detector': y1, 'picker_P': y2, 'picker_S': y3}: outputs including three separate numpy arrays as labels for detection, P, and S respectively.\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    def __init__(self, \n",
    "                 list_IDs, \n",
    "                 file_name, \n",
    "                 dim, \n",
    "                 batch_size=32, \n",
    "                 n_channels=3, \n",
    "                 norm_mode = 'max'):\n",
    "       \n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.file_name = file_name        \n",
    "        self.n_channels = n_channels\n",
    "        self.on_epoch_end()\n",
    "        self.norm_mode = norm_mode\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]           \n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        X = self.__data_generation(list_IDs_temp)\n",
    "        return ({'input': X})\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "    \n",
    "    def normalize(self, data, mode = 'max'):  \n",
    "        'Normalize waveforms in each batch'\n",
    "        \n",
    "        data -= np.mean(data, axis=0, keepdims=True)\n",
    "        if mode == 'max':\n",
    "            max_data = np.max(data, axis=0, keepdims=True)\n",
    "            assert(max_data.shape[-1] == data.shape[-1])\n",
    "            max_data[max_data == 0] = 1\n",
    "            data /= max_data              \n",
    "\n",
    "        elif mode == 'std':               \n",
    "            std_data = np.std(data, axis=0, keepdims=True)\n",
    "            assert(std_data.shape[-1] == data.shape[-1])\n",
    "            std_data[std_data == 0] = 1\n",
    "            data /= std_data\n",
    "        return data    \n",
    "\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'readint the waveforms' \n",
    "        \n",
    "        X = np.zeros((self.batch_size, self.dim, self.n_channels))\n",
    "        fl = h5py.File(self.file_name, 'r')\n",
    "        \n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            dataset = fl.get(str(ID))\n",
    "            data = np.array(dataset['data'])              \n",
    "\n",
    "          \n",
    "            if self.norm_mode:                    \n",
    "                data = self.normalize(data, self.norm_mode)  \n",
    "                            \n",
    "            X[i, :, :] = data                                       \n",
    "\n",
    "        fl.close() \n",
    "                           \n",
    "        return X\n",
    "    \n",
    "    \n",
    "def tester1(input_hdf5=None,\n",
    "           input_testset=None,\n",
    "           input_model=None,\n",
    "           output_name=None,\n",
    "           detection_threshold=0.20,                \n",
    "           P_threshold=0.1,\n",
    "           S_threshold=0.1, \n",
    "           number_of_plots=100,\n",
    "           estimate_uncertainty=True, \n",
    "           number_of_sampling=5,\n",
    "           loss_weights=[0.05, 0.40, 0.55],\n",
    "           loss_types=['mse'],\n",
    "           input_dimention=(6000, 3),\n",
    "           normalization_mode='std',\n",
    "           mode='generator',\n",
    "           batch_size=500,\n",
    "           gpuid=None,\n",
    "           gpu_limit=None):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Applies a trained model to a windowed waveform to perform both detection and picking at the same time.  \n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_hdf5: str, default=None\n",
    "        Path to an hdf5 file containing only one class of \"data\" with NumPy arrays containing 3 component waveforms each 1 min long.\n",
    "\n",
    "    input_testset: npy, default=None\n",
    "        Path to a NumPy file (automaticaly generated by the trainer) containing a list of trace names.        \n",
    "\n",
    "    input_model: str, default=None\n",
    "        Path to a trained model.\n",
    "        \n",
    "    output_dir: str, default=None\n",
    "        Output directory that will be generated. \n",
    "        \n",
    "    output_probabilities: bool, default=False\n",
    "        If True, it will output probabilities and estimated uncertainties for each trace into an HDF file. \n",
    "       \n",
    "          \n",
    "    P_threshold: float, default=0.1\n",
    "        A value which the P probabilities above it will be considered as P arrival.\n",
    "\n",
    "               \n",
    "    number_of_plots: float, default=10\n",
    "        The number of plots for detected events outputed for each station data.\n",
    "        \n",
    "    estimate_uncertainty: bool, default=False\n",
    "        If True uncertainties in the output probabilities will be estimated.  \n",
    "        \n",
    "    number_of_sampling: int, default=5\n",
    "        Number of sampling for the uncertainty estimation. \n",
    "               \n",
    "             \n",
    "    input_dimention: tuple, default=(6000, 3)\n",
    "        Loss types for P picking.          \n",
    "\n",
    "    normalization_mode: str, default='std' \n",
    "        Mode of normalization for data preprocessing, 'max', maximum amplitude among three components, 'std', standard deviation.\n",
    "\n",
    "    mode: str, default='generator'\n",
    "        Mode of running. 'pre_load_generator' or 'generator'.\n",
    "                      \n",
    "    batch_size: int, default=500 \n",
    "        Batch size. This wont affect the speed much but can affect the performance.\n",
    "\n",
    "    gpuid: int, default=None\n",
    "        Id of GPU used for the prediction. If using CPU set to None.\n",
    "         \n",
    "    gpu_limit: int, default=None\n",
    "        Set the maximum percentage of memory usage for the GPU.\n",
    "        \n",
    "\n",
    "    \"\"\" \n",
    "              \n",
    "         \n",
    "    args = {\n",
    "    \"input_hdf5\": input_hdf5,\n",
    "    \"input_testset\": input_testset,\n",
    "    \"input_model\": input_model,\n",
    "    \"output_name\": output_name,\n",
    "    \"detection_threshold\": detection_threshold,\n",
    "    \"P_threshold\": P_threshold,\n",
    "    \"number_of_plots\": number_of_plots,\n",
    "    \"estimate_uncertainty\": estimate_uncertainty,\n",
    "    \"number_of_sampling\": number_of_sampling,\n",
    "    \"input_dimention\": input_dimention,\n",
    "    \"normalization_mode\": normalization_mode,\n",
    "    \"mode\": mode,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"gpuid\": gpuid,\n",
    "    \"gpu_limit\": gpu_limit\n",
    "    }  \n",
    "\n",
    "    \n",
    "    if args['gpuid']:           \n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])\n",
    "        tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit']) \n",
    "        K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "    \n",
    "    save_dir = os.path.join(os.getcwd(), str(args['output_name'])+'_outputs')\n",
    "    save_figs = os.path.join(save_dir, 'figures')\n",
    " \n",
    "    if os.path.isdir(save_dir):\n",
    "        shutil.rmtree(save_dir)  \n",
    "    os.makedirs(save_figs) \n",
    " \n",
    "    test = np.load(args['input_testset'])\n",
    "    #test = test[0:10000]\n",
    "\n",
    "    print('Loading the model ...', flush=True) \n",
    "\n",
    "\n",
    "    \n",
    " # Model CCT\n",
    "    D1 = 5\n",
    "    D2 = int(D1*2)\n",
    "    D3 = int(D2*2)\n",
    "    D4 = int(D3*2)\n",
    "    D5 = int(D4*2)\n",
    "\n",
    "    inp = Input(shape=input_shapeX,name=\"input\")\n",
    "    conv1 = UNET(inp,D1)\n",
    "    out = Conv1D(D1,  3, strides =(1), padding='same',kernel_initializer='he_normal')(conv1)\n",
    "    out = Conv1D(3,  3, strides =(1), padding='same',kernel_initializer='he_normal',name='picker_PP')(out)\n",
    "    modeloriginal = Model(inp, out)\n",
    "\n",
    "    modeloriginal.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc',f1,precision, recall])     \n",
    "    modeloriginal.load_weights('../../../weights/UTrans_Foundation.h5')\n",
    "\n",
    "    # Model CCT\n",
    "    inputs = modeloriginal.layers[63].output  # layer that you want to connect your new FC layer to \n",
    "\n",
    "\n",
    "    featuresP = create_cct_modelP(inputs,inp)\n",
    "    featuresP = Reshape((6000,1))(featuresP)\n",
    "\n",
    "    logitp  = Conv1D(1,  3, strides =(1), padding='same',activation='sigmoid', kernel_initializer='he_normal',name='picker_P')(featuresP)\n",
    "\n",
    "    model = Model(inputs=[modeloriginal.input], outputs=logitp)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "          loss='binary_crossentropy',\n",
    "          metrics=['acc',f1,precision, recall]) \n",
    "\n",
    "        \n",
    "\n",
    "    model.load_weights(args['input_model'])\n",
    "\n",
    "\n",
    "    model.summary()  \n",
    "        \n",
    "    print('Loading is complete!', flush=True)  \n",
    "    print('Testing ...', flush=True)    \n",
    "    print('Writting results into: \" ' + str(args['output_name'])+'_outputs'+' \"', flush=True)\n",
    "    \n",
    "    start_training = time.time()          \n",
    "\n",
    "    csvTst = open(os.path.join(save_dir,'X_test_results.csv'), 'w')          \n",
    "    test_writer = csv.writer(csvTst, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    test_writer.writerow([\n",
    "                          \n",
    "                          \n",
    "                          'p_arrival_sample', \n",
    "                          \n",
    "\n",
    "                          \n",
    "                          'P_pick',\n",
    "                          'P_probability',\n",
    "                          'P_error'\n",
    "                          ])  \n",
    "    csvTst.flush()        \n",
    "        \n",
    "    plt_n = 0\n",
    "    list_generator = generate_arrays_from_file(test, args['batch_size']) \n",
    "    pred_PP_mean_all=[]\n",
    "    pred_PP_std_all=[]\n",
    "    sptall = []\n",
    "    \n",
    "    pbar_test = tqdm(total= int(np.ceil(len(test)/args['batch_size'])))            \n",
    "    for _ in range(int(np.ceil(len(test) / args['batch_size']))):\n",
    "        pbar_test.update()\n",
    "        new_list = next(list_generator)\n",
    "\n",
    "        if args['mode'].lower() == 'pre_load_generator':                \n",
    "            params_test = {'dim': args['input_dimention'][0],\n",
    "                           'batch_size': len(new_list),\n",
    "                           'n_channels': args['input_dimention'][-1],\n",
    "                           'norm_mode': args['normalization_mode']}  \n",
    "            test_set={}\n",
    "\n",
    "\n",
    "        \n",
    "        else:       \n",
    "            params_test = {'file_name': str(args['input_hdf5']), \n",
    "                           'dim': args['input_dimention'][0],\n",
    "                           'batch_size': len(new_list),\n",
    "                           'n_channels': args['input_dimention'][-1],\n",
    "                           'norm_mode': args['normalization_mode']}     \n",
    "    \n",
    "            test_generator = DataGeneratorTest(new_list, **params_test)\n",
    "            \n",
    "            if args['estimate_uncertainty']:\n",
    "                pred_PP = []\n",
    "                for mc in range(args['number_of_sampling']):\n",
    "                    predP = model.predict_generator(generator=test_generator)\n",
    "                    pred_PP.append(predP)\n",
    "                \n",
    "    \n",
    "                \n",
    "                pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])\n",
    "                pred_PP_mean = pred_PP.mean(axis=0)\n",
    "                pred_PP_std = pred_PP.std(axis=0) \n",
    "\n",
    "            else:          \n",
    "                pred_PP_mean = model.predict_generator(generator=test_generator)\n",
    "                pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1]) \n",
    "                \n",
    "                pred_PP_std = np.zeros((pred_PP_mean.shape))   \n",
    "                \n",
    "   \n",
    "            test_set={}\n",
    "            fl = h5py.File(args['input_hdf5'], 'r')\n",
    "            for ID in new_list:\n",
    "                dataset = fl.get(str(ID))\n",
    "                test_set.update( {str(ID) : dataset})                 \n",
    "            \n",
    "            for ts in range(pred_PP_mean.shape[0]): \n",
    "                evi =  new_list[ts] \n",
    "                dataset = test_set[evi]  \n",
    "                \n",
    "                    \n",
    "                try:\n",
    "                    spt = int(dataset.attrs['p_arrival_sample']);\n",
    "                except Exception:     \n",
    "                    spt = None\n",
    "                \n",
    "                \n",
    "                Ppick, perror, Pprob =  picker(args, pred_PP_mean[ts], pred_PP_std[ts], spt) \n",
    "                \n",
    "                _output_writter_test(args, dataset, evi, test_writer, csvTst, Ppick, perror, Pprob)\n",
    "                        \n",
    "                pred_PP_mean_all.append(pred_PP_mean[ts])\n",
    "                pred_PP_std_all.append(pred_PP_std[ts])\n",
    "                sptall.append(spt)\n",
    "\n",
    "    \n",
    "    np.save('pred_PP_mean_all_FoundationEQCCT.npy',pred_PP_mean_all)\n",
    "    np.save('pred_PP_std_all_FoundationEQCCT.npy',pred_PP_std_all)\n",
    "    np.save('pall_FoundationEQCCT.npy',sptall)\n",
    "    \n",
    "    end_training = time.time()  \n",
    "    delta = end_training - start_training\n",
    "    hour = int(delta / 3600)\n",
    "    delta -= hour * 3600\n",
    "    minute = int(delta / 60)\n",
    "    delta -= minute * 60\n",
    "    seconds = delta     \n",
    "                    \n",
    "    with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file: \n",
    "        the_file.write('================== Overal Info =============================='+'\\n')               \n",
    "                \n",
    "        the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\\n')            \n",
    "        the_file.write('input_testset: '+str(args['input_testset'])+'\\n')\n",
    "        the_file.write('input_model: '+str(args['input_model'])+'\\n')\n",
    "        the_file.write('output_name: '+str(args['output_name']+'_outputs')+'\\n')  \n",
    "        the_file.write('================== Testing Parameters ======================='+'\\n')  \n",
    "        the_file.write('mode: '+str(args['mode'])+'\\n')  \n",
    "        the_file.write('finished the test in:  {} hours and {} minutes and {} seconds \\n'.format(hour, minute, round(seconds, 3))) \n",
    "        the_file.write('batch_size: '+str(args['batch_size'])+'\\n')\n",
    "        the_file.write('total number of tests '+str(len(test))+'\\n')\n",
    "        the_file.write('gpuid: '+str(args['gpuid'])+'\\n')\n",
    "        the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\\n')             \n",
    "        the_file.write('================== Other Parameters ========================='+'\\n')            \n",
    "        the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\\n')\n",
    "        the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\\n')\n",
    "        the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\\n')                       \n",
    "        the_file.write('P_threshold: '+str(args['P_threshold'])+'\\n')\n",
    "        \n",
    "    \n",
    "    \n",
    "def _output_writter_test(args, \n",
    "                        dataset, \n",
    "                        evi, \n",
    "                        output_writer, \n",
    "                        csvfile, \n",
    "                        Ppick,\n",
    "                        perror,\n",
    "                        Pprob\n",
    "                        ):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Writes the detection & picking results into a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args: dic\n",
    "        A dictionary containing all of the input parameters.    \n",
    " \n",
    "    dataset: hdf5 obj\n",
    "        Dataset object of the trace.\n",
    "\n",
    "    evi: str\n",
    "        Trace name.    \n",
    "              \n",
    "    output_writer: obj\n",
    "        For writing out the detection/picking results in the CSV file.\n",
    "        \n",
    "    csvfile: obj\n",
    "        For writing out the detection/picking results in the CSV file.   \n",
    "             \n",
    "        \n",
    "    Returns\n",
    "    --------  \n",
    "    X_test_results.csv  \n",
    "    \n",
    "        \n",
    "    \"\"\"        \n",
    "    \n",
    "    \n",
    "    #print(dataset.attrs['data_category'] )\n",
    "    \n",
    "    if dataset.attrs['data_category'] != 'noise':                                     \n",
    "\n",
    "        p_arrival_sample = dataset.attrs['p_arrival_sample'] \n",
    "\n",
    "                   \n",
    "    elif dataset.attrs['data_category'] == 'noise':               \n",
    "        #network_code = dataset.attrs['network_code']\n",
    "        source_id = None\n",
    "        source_distance_km = None \n",
    "        snr_db = None\n",
    "        #trace_name = dataset.attrs['trace_name'] \n",
    "        #trace_category = dataset.attrs['trace_category']            \n",
    "        trace_start_time = None\n",
    "        source_magnitude = None\n",
    "        p_arrival_sample = None\n",
    "        p_status = None\n",
    "        p_weight = None\n",
    "        s_arrival_sample = None\n",
    "        s_status = None\n",
    "        s_weight = None\n",
    "        #receiver_type = dataset.attrs['receiver_type'] \n",
    "      \n",
    "    #print(Ppick[0])\n",
    "\n",
    "    output_writer.writerow([ \n",
    "\n",
    "                            p_arrival_sample, \n",
    "                             \n",
    "\n",
    "            \n",
    "                            Ppick[0], \n",
    "                            Pprob[0],\n",
    "                            perror[0],\n",
    "                            \n",
    "                            ]) \n",
    "    \n",
    "    csvfile.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f786b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#`input_model: Path to the trained model directory\n",
    "#`input_hdf5`: Path to the full waveform HDF5 file.\n",
    "\n",
    "tester1(input_hdf5= '/scratch/sadalyom/DataCollected',\n",
    "       input_testset='test_Events.npy',\n",
    "       input_model = 'test_trainer_P_FoundationEQCCT_outputs/models/',\n",
    "       output_name='test_tester_FoundationEQCCT',\n",
    "       detection_threshold=0.1,                \n",
    "       P_threshold=0.1,\n",
    "       number_of_plots=3,\n",
    "       estimate_uncertainty=False, \n",
    "       number_of_sampling=5,\n",
    "       input_dimention=(6000, 3),\n",
    "       normalization_mode='std',\n",
    "       mode='generator',\n",
    "       batch_size=1024,\n",
    "       gpuid=None,\n",
    "       gpu_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06307f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05ccb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd9d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Foundation",
   "language": "python",
   "name": "foundation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
